After converted

=====================
===params===
=====================
=====================
===modules===
          <class '__main__.QNet'>       : QNet(
  (model): Net(
    (conv1): QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
    (conv2): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
    (dropout1): Dropout(p=0.25, inplace=False)
    (dropout2): Dropout(p=0.5, inplace=False)
    (fc1): QuantizedLinear(in_features=9216, out_features=128, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
    (fc2): QuantizedLinear(in_features=128, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
  )
)
---------------------------model     <class '__main__.Net'>        : Net(
  (conv1): QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
  (conv2): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
  (dropout1): Dropout(p=0.25, inplace=False)
  (dropout2): Dropout(p=0.5, inplace=False)
  (fc1): QuantizedLinear(in_features=9216, out_features=128, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
  (fc2): QuantizedLinear(in_features=128, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
)
---------------------------model.conv1<class 'torch.nn.quantized.modules.conv.Conv2d'>: QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
---------------------------model.conv2<class 'torch.nn.quantized.modules.conv.Conv2d'>: QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
---------------------------model.dropout1<class 'torch.nn.modules.dropout.Dropout'>: Dropout(p=0.25, inplace=False)
---------------------------model.dropout2<class 'torch.nn.modules.dropout.Dropout'>: Dropout(p=0.5, inplace=False)
---------------------------model.fc1 <class 'torch.nn.quantized.modules.linear.Linear'>: QuantizedLinear(in_features=9216, out_features=128, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
---------------------------model.fc1._packed_params<class 'torch.nn.quantized.modules.linear.LinearPackedParams'>: (tensor([[ 0.0127, -0.0070,  0.0042,  ..., -0.0183,  0.0070,  0.0141],
        [ 0.0000, -0.0084,  0.0169,  ..., -0.0479, -0.0239,  0.0113],
        [ 0.0099,  0.0014, -0.0084,  ..., -0.0141, -0.0113,  0.0042],
        ...,
        [-0.0042, -0.0014,  0.0239,  ..., -0.0127, -0.0197, -0.0084],
        [-0.0099, -0.0099, -0.0099,  ...,  0.0042, -0.0056,  0.0028],
        [-0.0056,  0.0070, -0.0042,  ..., -0.0211,  0.0000, -0.0127]],
       size=(128, 9216), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0014075974468141794,
       zero_point=0), tensor([-1.7558e-03, -2.5703e-02,  2.4065e-03,  1.0916e-02, -7.9553e-03,
        -1.4218e-02, -6.9479e-03,  2.0471e-02, -1.9638e-02, -7.4517e-03,
         2.1780e-03, -1.8941e-03,  2.4091e-02, -1.1227e-02, -1.4752e-02,
         3.5139e-04, -9.7976e-03, -1.2322e-02, -5.4179e-03, -1.4609e-02,
        -3.3809e-03, -4.6309e-03,  1.5808e-02, -1.3797e-02, -1.5176e-02,
         3.3732e-03, -2.1074e-02, -1.8279e-02,  1.5112e-03, -8.4515e-03,
         1.4966e-02,  1.6066e-02, -9.6311e-03,  4.2724e-05, -1.4623e-02,
         7.9703e-03, -5.2246e-03, -1.7503e-03, -1.8908e-02, -9.0204e-03,
         6.0572e-04, -1.3023e-02,  1.6581e-02, -6.4040e-03,  6.4666e-03,
         1.0487e-02,  1.1113e-02, -1.9785e-02,  1.5550e-03, -1.6939e-02,
         5.5166e-03,  7.3902e-03,  1.1096e-02, -9.6075e-03, -8.8328e-03,
        -4.7027e-04, -1.2491e-02,  4.2954e-03, -1.1657e-03, -8.8121e-03,
         1.3158e-02,  7.6111e-03, -1.0439e-02,  9.5565e-03, -3.4633e-03,
        -1.8689e-02, -7.2098e-03, -7.3144e-03, -1.2027e-02, -3.6868e-03,
         9.5429e-04, -7.1390e-03, -2.1284e-02,  2.2477e-02,  2.2013e-04,
        -3.9612e-03,  1.5451e-03, -3.1834e-03, -5.1276e-03, -1.3023e-02,
         1.7766e-02,  2.4163e-02,  2.3508e-02, -1.7727e-02, -9.1485e-05,
        -1.5576e-02, -1.1030e-02, -2.5206e-03, -1.2140e-02, -1.0998e-02,
        -2.1517e-02, -8.4767e-03,  7.2220e-03,  9.5459e-03,  2.8135e-02,
        -2.5022e-03,  1.0957e-02, -4.2475e-02, -2.1257e-02,  2.2480e-03,
        -8.1304e-04, -2.2216e-02,  3.3460e-03, -1.2243e-03, -6.6037e-04,
        -1.9586e-02, -2.0984e-02, -6.9473e-03, -2.9654e-02, -1.0762e-02,
        -1.0114e-02,  1.0635e-02,  3.2545e-04,  3.0234e-03, -5.8417e-03,
         9.8255e-03, -1.6776e-03, -1.2833e-02, -1.0766e-02, -1.4368e-02,
        -1.7255e-02, -9.2194e-03,  2.4796e-04,  4.7861e-03, -1.1339e-02,
        -9.4369e-03, -7.7872e-03, -1.7708e-03], requires_grad=True))
---------------------------model.fc2 <class 'torch.nn.quantized.modules.linear.Linear'>: QuantizedLinear(in_features=128, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
---------------------------model.fc2._packed_params<class 'torch.nn.quantized.modules.linear.LinearPackedParams'>: (tensor([[-0.1342, -0.1070, -0.0015,  ..., -0.1116,  0.0151,  0.0256],
        [-0.0904, -0.0301,  0.0317,  ...,  0.0392,  0.0045, -0.0543],
        [-0.0678, -0.0769, -0.0513,  ..., -0.0950, -0.0814, -0.0136],
        ...,
        [ 0.0497, -0.0543,  0.0754,  ..., -0.0739, -0.0121,  0.0708],
        [ 0.0151, -0.0784, -0.0317,  ...,  0.0558,  0.0226,  0.0030],
        [ 0.0332, -0.0362,  0.0106,  ...,  0.0558,  0.0482,  0.0362]],
       size=(10, 128), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0015074362745508552,
       zero_point=0), tensor([-0.0504,  0.1116,  0.0299, -0.0010,  0.0487, -0.0432,  0.0310, -0.0144,
        -0.0493,  0.0225], requires_grad=True))
---------------------------After converted

=====================
===params===
model.conv1.weight<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([[[[-2.9880e-01, -1.0493e-01, -9.4408e-02],
          [-4.0809e-03, -9.0901e-02, -2.3946e-01],
          [ 7.0191e-03, -2.4777e-01,  5.5444e-02]]],


        [[[-1.4003e-01,  5.9618e-02, -2.8677e-01],
          [-6.7451e-02,  1.6354e-01,  3.4268e-01],
          [ 1.2182e-01, -1.9744e-01, -1.4641e-01]]],


        [[[ 3.3644e-01,  1.5786e-01,  1.6438e-01],
          [ 2.1964e-01,  3.6397e-01,  2.4788e-02],
          [-4.1152e-02,  9.3328e-02,  2.8832e-01]]],


        [[[ 3.0041e-01, -1.5248e-01,  1.7188e-01],
          [ 3.5256e-01,  1.8857e-01, -6.5669e-02],
          [ 3.0654e-02, -2.5478e-01,  1.2163e-01]]],


        [[[ 1.2081e-01, -1.4371e-01, -4.0632e-01],
          [ 9.5525e-02,  3.0044e-01, -6.6426e-02],
          [ 4.4631e-01,  3.5866e-01,  3.6507e-01]]],


        [[[ 3.2748e-01,  3.0948e-02,  2.6961e-01],
          [ 5.1997e-02, -1.9818e-01,  4.2359e-01],
          [-3.5331e-01, -2.7336e-01,  1.6425e-01]]],


        [[[ 1.3710e-01, -1.0606e-02,  4.5137e-02],
          [ 1.6811e-01,  2.9117e-01,  3.2487e-01],
          [-2.9893e-01, -2.7467e-01, -4.1834e-01]]],


        [[[-1.4843e-01, -2.3468e-01,  9.4389e-02],
          [-6.8487e-02, -1.2644e-01, -1.9385e-02],
          [-3.3218e-01, -1.5136e-01, -7.6472e-02]]],


        [[[ 1.6482e-01,  3.1565e-01,  3.7848e-02],
          [ 1.7079e-01,  3.0124e-01, -8.2340e-02],
          [-6.2145e-02, -3.6574e-01, -3.9719e-01]]],


        [[[-1.2120e-01, -4.2474e-01, -3.7961e-04],
          [ 2.2245e-03, -3.5148e-01,  1.1253e-01],
          [-3.5961e-01, -1.7102e-01,  3.4422e-01]]],


        [[[ 9.2521e-02, -2.3701e-01, -1.0267e-01],
          [-2.2923e-01,  5.2078e-02, -1.4016e-01],
          [-3.0949e-01, -3.2445e-01,  1.1963e-01]]],


        [[[-9.6144e-02,  2.5533e-01,  2.6186e-01],
          [-3.7505e-01, -1.6370e-01,  4.4318e-01],
          [-3.1788e-01, -1.7731e-01,  4.0111e-01]]],


        [[[ 1.1919e-01,  1.3126e-01,  1.7953e-02],
          [-3.5331e-02,  1.2237e-01,  2.0066e-01],
          [-1.8288e-01, -4.3487e-01,  3.4378e-02]]],


        [[[ 7.6056e-02,  3.4161e-01,  1.3664e-01],
          [-1.6394e-01, -1.3505e-01, -1.1138e-01],
          [ 7.2916e-02,  6.5297e-02,  2.2525e-01]]],


        [[[-3.3831e-01, -4.5490e-01, -3.6624e-01],
          [ 1.4136e-01,  1.7022e-01,  1.3857e-01],
          [ 1.9665e-01,  2.7525e-01, -9.9629e-02]]],


        [[[-5.0057e-02, -3.6314e-02,  1.2715e-01],
          [-2.7514e-01,  7.9140e-02,  2.9155e-01],
          [ 1.2797e-01, -3.5860e-01,  1.2535e-01]]],


        [[[ 5.7037e-02, -2.3822e-01,  5.7111e-02],
          [ 1.0360e-01,  1.5539e-01,  6.3637e-02],
          [ 1.5844e-01,  2.7834e-01, -1.7206e-01]]],


        [[[-8.9117e-02, -1.6373e-02,  1.7731e-01],
          [ 3.0791e-01,  9.3516e-02,  3.1589e-01],
          [ 9.7591e-02,  1.6531e-01,  7.1941e-02]]],


        [[[-2.3436e-01,  2.6186e-02, -1.0402e-01],
          [-3.6678e-01,  1.3026e-01, -2.9739e-01],
          [-1.5570e-01, -1.0743e-01, -8.0777e-02]]],


        [[[-1.1433e-02, -2.4903e-01,  1.3219e-01],
          [-2.8103e-01, -1.7838e-01, -9.1438e-02],
          [-3.8548e-01, -1.5026e-01, -7.6040e-02]]],


        [[[-3.1344e-01,  6.9156e-02,  1.1146e-01],
          [-2.9101e-01,  3.4138e-01,  9.3271e-02],
          [ 8.2602e-03,  2.0942e-01,  2.7972e-01]]],


        [[[-3.6355e-01, -9.8365e-02,  3.2882e-01],
          [-6.8063e-02,  5.3917e-02,  8.0882e-02],
          [-1.6333e-01,  4.0149e-01,  2.8698e-01]]],


        [[[ 3.8086e-01,  6.5091e-02,  3.2578e-01],
          [-1.4561e-01,  2.7501e-01, -1.6371e-01],
          [ 1.5826e-01,  1.4048e-01, -2.1911e-01]]],


        [[[ 2.7663e-01,  3.0411e-01, -1.8270e-01],
          [ 1.0251e-01, -2.3115e-02, -2.9186e-01],
          [ 2.2537e-01, -2.6347e-01, -2.7846e-01]]],


        [[[ 1.2907e-01,  2.4934e-01,  3.7553e-01],
          [ 2.4604e-01,  2.3326e-01, -2.9987e-02],
          [ 1.3922e-01,  2.9301e-01, -2.0738e-01]]],


        [[[ 9.8661e-02, -1.7491e-02,  8.9968e-03],
          [ 2.0598e-01,  5.6547e-02,  3.7761e-01],
          [-2.8021e-01, -1.7694e-01, -2.4932e-02]]],


        [[[ 2.4616e-01, -2.8673e-01, -2.7213e-01],
          [-1.4000e-01, -3.9073e-01,  4.4300e-02],
          [-8.8992e-02, -1.9969e-01,  2.5306e-01]]],


        [[[-2.9159e-01, -2.3097e-01, -3.6997e-01],
          [-1.5458e-01, -1.4304e-01, -2.7364e-03],
          [ 2.1527e-01,  2.5703e-01,  3.2883e-02]]],


        [[[ 2.6537e-01, -1.8020e-02, -3.2031e-01],
          [-3.2984e-02,  8.0872e-02, -2.7292e-01],
          [-8.4592e-02,  1.6451e-02, -4.3469e-02]]],


        [[[ 2.1441e-01,  3.7469e-01, -1.1640e-01],
          [ 3.2611e-01,  1.4580e-01,  3.2722e-01],
          [-1.9956e-01, -2.8273e-01, -1.3855e-01]]],


        [[[ 2.2820e-01,  2.4348e-01,  2.0774e-01],
          [-5.5315e-02,  3.2912e-01,  3.0041e-01],
          [ 1.7732e-01, -3.3415e-01, -2.5708e-01]]],


        [[[-1.5884e-01,  1.4521e-01,  2.2761e-01],
          [-1.0270e-01, -3.8092e-01, -3.3882e-01],
          [ 3.0715e-02,  1.1327e-01, -5.8706e-02]]]], requires_grad=True)
---------------------------
model.conv1.bias<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([-0.2409,  0.0724,  0.0955, -0.0036,  0.1251,  0.0163,  0.1566, -0.3302,
         0.3033,  0.2697, -0.1810,  0.2523,  0.1834, -0.0104,  0.2157,  0.2366,
         0.0101,  0.0833,  0.2126,  0.3820,  0.0117, -0.0878, -0.1076,  0.0744,
        -0.0380,  0.0126,  0.1906,  0.3059, -0.3417,  0.0423, -0.0004,  0.1466],
       requires_grad=True)
---------------------------
model.conv2.weight<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([[[[ 2.2251e-02, -4.5430e-02, -2.5386e-02],
          [-2.8277e-02,  4.4625e-02, -1.9133e-02],
          [ 1.7407e-02, -2.9140e-02, -4.6816e-02]],

         [[ 6.0873e-02,  1.5478e-01,  6.5877e-02],
          [ 5.7981e-02,  2.9333e-02, -3.7363e-03],
          [-2.6730e-02, -6.8750e-02,  5.8222e-02]],

         [[-3.1369e-02, -9.1750e-03,  4.4650e-02],
          [-4.3660e-03, -1.7365e-02, -1.6397e-02],
          [-4.6239e-02, -7.9950e-02, -2.3298e-02]],

         ...,

         [[-3.7226e-02, -1.6294e-02,  6.0316e-02],
          [ 8.8456e-03,  6.7735e-02,  8.0788e-03],
          [-1.8276e-02,  3.8795e-02,  2.5336e-02]],

         [[-4.1092e-02,  5.2458e-02,  5.3392e-02],
          [ 6.5459e-02, -3.0011e-02,  4.2680e-02],
          [-8.8483e-03,  2.3632e-02, -7.5326e-02]],

         [[ 5.2193e-02, -9.7431e-03,  3.6064e-03],
          [ 6.7965e-02,  5.6165e-02,  5.7454e-02],
          [ 1.2170e-01,  3.2655e-02,  4.1076e-02]]],


        [[[ 2.4213e-02,  2.3419e-03,  5.5778e-02],
          [ 8.4009e-03, -1.3928e-02, -3.9568e-02],
          [ 2.1530e-02, -2.9712e-02, -5.2443e-02]],

         [[-3.6052e-02,  5.2014e-02, -4.8413e-02],
          [-4.5626e-02,  5.3182e-03, -7.9157e-02],
          [ 1.6329e-02, -6.7772e-02,  1.7193e-02]],

         [[ 2.3961e-02,  2.9844e-02,  6.8519e-02],
          [ 4.3712e-02,  3.6313e-02,  5.0415e-02],
          [ 2.2500e-02,  6.2322e-02,  5.4046e-02]],

         ...,

         [[-9.1779e-02,  3.1340e-02,  5.7061e-02],
          [-8.1271e-02, -3.6358e-02, -1.5579e-02],
          [ 4.0008e-02, -9.2390e-03, -4.9136e-02]],

         [[ 7.2060e-03,  9.8608e-03, -9.2604e-03],
          [ 1.6913e-02,  4.4102e-02, -3.8534e-02],
          [ 2.9955e-02, -5.2278e-03,  5.9734e-02]],

         [[-6.4326e-02, -9.8949e-02, -3.3211e-02],
          [-7.6599e-02,  1.4234e-02, -7.5904e-03],
          [-8.2197e-03, -6.9464e-02, -6.9155e-02]]],


        [[[-7.3226e-03,  5.0315e-02,  1.9218e-02],
          [ 5.3646e-03, -2.0761e-02, -5.3933e-02],
          [ 1.8797e-02,  2.9451e-02,  1.6925e-02]],

         [[-5.9609e-02,  2.1142e-03,  3.4217e-02],
          [-2.4323e-02, -2.1229e-02, -2.0598e-02],
          [-3.6279e-02,  3.4073e-02, -3.2888e-02]],

         [[-4.2278e-02, -5.6760e-02, -2.5758e-02],
          [-4.2710e-02, -3.0216e-03, -3.8033e-02],
          [-2.0301e-02, -3.5858e-02,  2.6355e-02]],

         ...,

         [[ 3.9693e-02, -2.0976e-02, -1.3860e-02],
          [ 5.2524e-02, -3.8656e-02,  5.1280e-02],
          [ 2.6810e-02, -3.7756e-02, -1.7099e-02]],

         [[ 2.8485e-02, -5.7918e-02, -2.1931e-02],
          [-8.2517e-03, -3.3711e-02,  1.9482e-02],
          [-1.5365e-02,  2.4832e-02,  4.9834e-02]],

         [[-2.7950e-03,  1.6670e-03, -3.3011e-02],
          [-3.5629e-02, -6.2907e-02,  5.2544e-03],
          [-1.4581e-02, -5.5157e-02, -3.7421e-02]]],


        ...,


        [[[-1.2765e-02,  3.1553e-02,  2.9452e-02],
          [-1.8599e-02, -5.7710e-02,  4.9460e-02],
          [-4.3315e-02,  2.0651e-02, -1.9595e-02]],

         [[ 4.5884e-02,  9.6146e-03, -1.1508e-01],
          [ 4.8294e-02, -4.2040e-02, -1.5631e-01],
          [-3.8786e-02, -8.1839e-02, -1.4454e-01]],

         [[-8.5455e-02, -3.4115e-02,  6.4393e-02],
          [-1.3080e-02, -5.3862e-02,  1.1066e-02],
          [-8.0429e-02,  4.7378e-02, -1.2956e-02]],

         ...,

         [[-9.4439e-02, -1.0833e-01, -6.1762e-02],
          [-5.3487e-02, -9.0517e-02, -4.8512e-02],
          [-8.6582e-02, -3.7222e-02, -7.6154e-02]],

         [[-1.7652e-01, -4.4356e-02, -3.6280e-03],
          [-6.8133e-02, -7.0354e-02, -5.1152e-02],
          [-5.5210e-02, -3.5989e-02,  3.4846e-02]],

         [[-5.6106e-02, -6.2926e-02, -5.5950e-02],
          [ 2.5118e-02, -1.0752e-01, -1.2758e-01],
          [-6.4627e-03, -5.0722e-02, -1.2945e-01]]],


        [[[-8.9633e-03,  4.2713e-02, -1.7727e-02],
          [-1.2763e-02,  1.4326e-02,  4.9100e-02],
          [-3.3700e-02, -6.1453e-03,  6.7158e-03]],

         [[-5.0387e-02,  3.2007e-02, -3.0527e-02],
          [ 2.8820e-02,  2.8694e-02,  2.1336e-02],
          [ 4.6339e-02,  2.9584e-02,  3.0142e-02]],

         [[-1.1583e-01,  6.4881e-03, -2.7621e-02],
          [-1.5559e-02, -1.2821e-02, -3.1000e-03],
          [ 1.6956e-02,  5.0312e-02, -1.5349e-02]],

         ...,

         [[-1.2875e-01, -1.0522e-01, -1.2133e-02],
          [-5.7523e-02, -5.6372e-02, -3.3886e-02],
          [ 7.8355e-02,  6.5530e-02,  2.1318e-02]],

         [[-1.7006e-01, -1.2965e-01, -8.1324e-02],
          [-1.4104e-01, -1.2573e-01, -1.2902e-01],
          [ 3.5625e-02, -4.4216e-04, -1.1563e-01]],

         [[ 3.0868e-02,  9.9898e-03,  3.2969e-03],
          [ 6.2428e-02,  7.0464e-02,  1.4789e-02],
          [-3.2964e-02, -2.5214e-02,  2.7996e-02]]],


        [[[ 3.4728e-02, -3.8239e-02, -3.4733e-03],
          [-3.0558e-02,  1.9031e-02, -2.0839e-02],
          [ 3.8965e-02, -1.9313e-03, -1.5811e-02]],

         [[ 7.9999e-03,  9.3312e-03, -7.6349e-02],
          [-5.1404e-02, -6.3635e-02,  3.3509e-03],
          [ 1.6181e-03,  3.5794e-02, -2.5207e-02]],

         [[ 3.3059e-02,  3.9020e-02, -9.6901e-03],
          [-1.4938e-02, -3.5016e-02, -4.9876e-02],
          [-3.9024e-02, -4.1290e-03, -4.8286e-02]],

         ...,

         [[ 7.1011e-02, -2.4446e-02,  2.7939e-02],
          [ 5.1157e-02, -2.3582e-02,  5.2506e-02],
          [ 2.6109e-02, -4.7219e-02, -1.9183e-02]],

         [[ 5.1226e-02, -3.5926e-02, -2.0594e-02],
          [ 7.0981e-02, -2.0965e-02, -1.3958e-04],
          [-5.7459e-02,  2.8277e-02, -8.3378e-02]],

         [[ 5.1026e-02,  5.6739e-02,  2.3853e-02],
          [-8.6888e-03, -8.1049e-02, -5.6273e-02],
          [ 2.6133e-02,  3.6434e-02,  3.0561e-02]]]], requires_grad=True)
---------------------------
model.conv2.bias<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([-0.0616, -0.0706, -0.0554,  0.0287, -0.0064, -0.0171, -0.0361, -0.0463,
        -0.0159,  0.0071, -0.0384,  0.0423, -0.0419, -0.0094, -0.0238,  0.0441,
         0.0172, -0.0497, -0.0217, -0.0087, -0.0494, -0.0023,  0.0062,  0.0092,
         0.0403,  0.0418,  0.0262, -0.0091,  0.0046,  0.0010, -0.0392,  0.0577,
        -0.0124,  0.0236, -0.0366, -0.0457, -0.0210,  0.0138, -0.0445, -0.0681,
        -0.0461, -0.0559,  0.0473, -0.0356, -0.0414, -0.0642, -0.0269,  0.0103,
         0.0154, -0.0047, -0.0407, -0.0656,  0.0472,  0.0462, -0.0323, -0.0071,
         0.0021,  0.0459,  0.0097, -0.0419, -0.0322,  0.0304,  0.0535, -0.0257],
       requires_grad=True)
---------------------------
model.fc1.weight<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([[ 0.0127, -0.0064,  0.0038,  ..., -0.0185,  0.0077,  0.0141],
        [ 0.0001, -0.0089,  0.0171,  ..., -0.0478, -0.0235,  0.0117],
        [ 0.0100,  0.0019, -0.0089,  ..., -0.0145, -0.0115,  0.0043],
        ...,
        [-0.0039, -0.0016,  0.0238,  ..., -0.0132, -0.0193, -0.0088],
        [-0.0103, -0.0092, -0.0098,  ...,  0.0042, -0.0060,  0.0035],
        [-0.0059,  0.0066, -0.0040,  ..., -0.0218, -0.0002, -0.0123]],
       requires_grad=True)
---------------------------
model.fc1.bias<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([-1.7558e-03, -2.5703e-02,  2.4065e-03,  1.0916e-02, -7.9553e-03,
        -1.4218e-02, -6.9479e-03,  2.0471e-02, -1.9638e-02, -7.4517e-03,
         2.1780e-03, -1.8941e-03,  2.4091e-02, -1.1227e-02, -1.4752e-02,
         3.5139e-04, -9.7976e-03, -1.2322e-02, -5.4179e-03, -1.4609e-02,
        -3.3809e-03, -4.6309e-03,  1.5808e-02, -1.3797e-02, -1.5176e-02,
         3.3732e-03, -2.1074e-02, -1.8279e-02,  1.5112e-03, -8.4515e-03,
         1.4966e-02,  1.6066e-02, -9.6311e-03,  4.2724e-05, -1.4623e-02,
         7.9703e-03, -5.2246e-03, -1.7503e-03, -1.8908e-02, -9.0204e-03,
         6.0572e-04, -1.3023e-02,  1.6581e-02, -6.4040e-03,  6.4666e-03,
         1.0487e-02,  1.1113e-02, -1.9785e-02,  1.5550e-03, -1.6939e-02,
         5.5166e-03,  7.3902e-03,  1.1096e-02, -9.6075e-03, -8.8328e-03,
        -4.7027e-04, -1.2491e-02,  4.2954e-03, -1.1657e-03, -8.8121e-03,
         1.3158e-02,  7.6111e-03, -1.0439e-02,  9.5565e-03, -3.4633e-03,
        -1.8689e-02, -7.2098e-03, -7.3144e-03, -1.2027e-02, -3.6868e-03,
         9.5429e-04, -7.1390e-03, -2.1284e-02,  2.2477e-02,  2.2013e-04,
        -3.9612e-03,  1.5451e-03, -3.1834e-03, -5.1276e-03, -1.3023e-02,
         1.7766e-02,  2.4163e-02,  2.3508e-02, -1.7727e-02, -9.1485e-05,
        -1.5576e-02, -1.1030e-02, -2.5206e-03, -1.2140e-02, -1.0998e-02,
        -2.1517e-02, -8.4767e-03,  7.2220e-03,  9.5459e-03,  2.8135e-02,
        -2.5022e-03,  1.0957e-02, -4.2475e-02, -2.1257e-02,  2.2480e-03,
        -8.1304e-04, -2.2216e-02,  3.3460e-03, -1.2243e-03, -6.6037e-04,
        -1.9586e-02, -2.0984e-02, -6.9473e-03, -2.9654e-02, -1.0762e-02,
        -1.0114e-02,  1.0635e-02,  3.2545e-04,  3.0234e-03, -5.8417e-03,
         9.8255e-03, -1.6776e-03, -1.2833e-02, -1.0766e-02, -1.4368e-02,
        -1.7255e-02, -9.2194e-03,  2.4796e-04,  4.7861e-03, -1.1339e-02,
        -9.4369e-03, -7.7872e-03, -1.7708e-03], requires_grad=True)
---------------------------
model.fc2.weight<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([[-0.1344, -0.1065, -0.0022,  ..., -0.1111,  0.0153,  0.0260],
        [-0.0899, -0.0298,  0.0323,  ...,  0.0389,  0.0042, -0.0541],
        [-0.0673, -0.0771, -0.0515,  ..., -0.0944, -0.0817, -0.0136],
        ...,
        [ 0.0498, -0.0536,  0.0760,  ..., -0.0737, -0.0126,  0.0705],
        [ 0.0152, -0.0784, -0.0312,  ...,  0.0553,  0.0222,  0.0036],
        [ 0.0325, -0.0365,  0.0105,  ...,  0.0558,  0.0487,  0.0369]],
       requires_grad=True)
---------------------------
model.fc2.bias<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([-0.0504,  0.1116,  0.0299, -0.0010,  0.0487, -0.0432,  0.0310, -0.0144,
        -0.0493,  0.0225], requires_grad=True)
---------------------------
===========================
===========================
===modules===
          <class '__main__.QNet'>       : QNet(
  (model): Net(
    (conv1): Conv2d(
      1, 32, kernel_size=(3, 3), stride=(1, 1)
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
    (conv2): Conv2d(
      32, 64, kernel_size=(3, 3), stride=(1, 1)
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
    (dropout1): Dropout(p=0.25, inplace=False)
    (dropout2): Dropout(p=0.5, inplace=False)
    (fc1): Linear(
      in_features=9216, out_features=128, bias=True
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
    (fc2): Linear(
      in_features=128, out_features=10, bias=True
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
)
---------------------------model     <class '__main__.Net'>        : Net(
  (conv1): Conv2d(
    1, 32, kernel_size=(3, 3), stride=(1, 1)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (conv2): Conv2d(
    32, 64, kernel_size=(3, 3), stride=(1, 1)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (dropout1): Dropout(p=0.25, inplace=False)
  (dropout2): Dropout(p=0.5, inplace=False)
  (fc1): Linear(
    in_features=9216, out_features=128, bias=True
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (fc2): Linear(
    in_features=128, out_features=10, bias=True
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
)
---------------------------model.conv1<class 'torch.nn.modules.conv.Conv2d'>: Conv2d(
  1, 32, kernel_size=(3, 3), stride=(1, 1)
  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
)
---------------------------model.conv1.activation_post_process<class 'torch.quantization.observer.MinMaxObserver'>: MinMaxObserver(min_val=inf, max_val=-inf)
---------------------------model.conv2<class 'torch.nn.modules.conv.Conv2d'>: Conv2d(
  32, 64, kernel_size=(3, 3), stride=(1, 1)
  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
)
---------------------------model.conv2.activation_post_process<class 'torch.quantization.observer.MinMaxObserver'>: MinMaxObserver(min_val=inf, max_val=-inf)
---------------------------model.dropout1<class 'torch.nn.modules.dropout.Dropout'>: Dropout(p=0.25, inplace=False)
---------------------------model.dropout2<class 'torch.nn.modules.dropout.Dropout'>: Dropout(p=0.5, inplace=False)
---------------------------model.fc1 <class 'torch.nn.modules.linear.Linear'>: Linear(
  in_features=9216, out_features=128, bias=True
  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
)
---------------------------model.fc1.activation_post_process<class 'torch.quantization.observer.MinMaxObserver'>: MinMaxObserver(min_val=inf, max_val=-inf)
---------------------------model.fc2 <class 'torch.nn.modules.linear.Linear'>: Linear(
  in_features=128, out_features=10, bias=True
  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
)
---------------------------model.fc2.activation_post_process<class 'torch.quantization.observer.MinMaxObserver'>: MinMaxObserver(min_val=inf, max_val=-inf)
---------------------------After converted

=====================
===params===
model.conv1.weight<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([[[[-2.9880e-01, -1.0493e-01, -9.4408e-02],
          [-4.0809e-03, -9.0901e-02, -2.3946e-01],
          [ 7.0191e-03, -2.4777e-01,  5.5444e-02]]],


        [[[-1.4003e-01,  5.9618e-02, -2.8677e-01],
          [-6.7451e-02,  1.6354e-01,  3.4268e-01],
          [ 1.2182e-01, -1.9744e-01, -1.4641e-01]]],


        [[[ 3.3644e-01,  1.5786e-01,  1.6438e-01],
          [ 2.1964e-01,  3.6397e-01,  2.4788e-02],
          [-4.1152e-02,  9.3328e-02,  2.8832e-01]]],


        [[[ 3.0041e-01, -1.5248e-01,  1.7188e-01],
          [ 3.5256e-01,  1.8857e-01, -6.5669e-02],
          [ 3.0654e-02, -2.5478e-01,  1.2163e-01]]],


        [[[ 1.2081e-01, -1.4371e-01, -4.0632e-01],
          [ 9.5525e-02,  3.0044e-01, -6.6426e-02],
          [ 4.4631e-01,  3.5866e-01,  3.6507e-01]]],


        [[[ 3.2748e-01,  3.0948e-02,  2.6961e-01],
          [ 5.1997e-02, -1.9818e-01,  4.2359e-01],
          [-3.5331e-01, -2.7336e-01,  1.6425e-01]]],


        [[[ 1.3710e-01, -1.0606e-02,  4.5137e-02],
          [ 1.6811e-01,  2.9117e-01,  3.2487e-01],
          [-2.9893e-01, -2.7467e-01, -4.1834e-01]]],


        [[[-1.4843e-01, -2.3468e-01,  9.4389e-02],
          [-6.8487e-02, -1.2644e-01, -1.9385e-02],
          [-3.3218e-01, -1.5136e-01, -7.6472e-02]]],


        [[[ 1.6482e-01,  3.1565e-01,  3.7848e-02],
          [ 1.7079e-01,  3.0124e-01, -8.2340e-02],
          [-6.2145e-02, -3.6574e-01, -3.9719e-01]]],


        [[[-1.2120e-01, -4.2474e-01, -3.7961e-04],
          [ 2.2245e-03, -3.5148e-01,  1.1253e-01],
          [-3.5961e-01, -1.7102e-01,  3.4422e-01]]],


        [[[ 9.2521e-02, -2.3701e-01, -1.0267e-01],
          [-2.2923e-01,  5.2078e-02, -1.4016e-01],
          [-3.0949e-01, -3.2445e-01,  1.1963e-01]]],


        [[[-9.6144e-02,  2.5533e-01,  2.6186e-01],
          [-3.7505e-01, -1.6370e-01,  4.4318e-01],
          [-3.1788e-01, -1.7731e-01,  4.0111e-01]]],


        [[[ 1.1919e-01,  1.3126e-01,  1.7953e-02],
          [-3.5331e-02,  1.2237e-01,  2.0066e-01],
          [-1.8288e-01, -4.3487e-01,  3.4378e-02]]],


        [[[ 7.6056e-02,  3.4161e-01,  1.3664e-01],
          [-1.6394e-01, -1.3505e-01, -1.1138e-01],
          [ 7.2916e-02,  6.5297e-02,  2.2525e-01]]],


        [[[-3.3831e-01, -4.5490e-01, -3.6624e-01],
          [ 1.4136e-01,  1.7022e-01,  1.3857e-01],
          [ 1.9665e-01,  2.7525e-01, -9.9629e-02]]],


        [[[-5.0057e-02, -3.6314e-02,  1.2715e-01],
          [-2.7514e-01,  7.9140e-02,  2.9155e-01],
          [ 1.2797e-01, -3.5860e-01,  1.2535e-01]]],


        [[[ 5.7037e-02, -2.3822e-01,  5.7111e-02],
          [ 1.0360e-01,  1.5539e-01,  6.3637e-02],
          [ 1.5844e-01,  2.7834e-01, -1.7206e-01]]],


        [[[-8.9117e-02, -1.6373e-02,  1.7731e-01],
          [ 3.0791e-01,  9.3516e-02,  3.1589e-01],
          [ 9.7591e-02,  1.6531e-01,  7.1941e-02]]],


        [[[-2.3436e-01,  2.6186e-02, -1.0402e-01],
          [-3.6678e-01,  1.3026e-01, -2.9739e-01],
          [-1.5570e-01, -1.0743e-01, -8.0777e-02]]],


        [[[-1.1433e-02, -2.4903e-01,  1.3219e-01],
          [-2.8103e-01, -1.7838e-01, -9.1438e-02],
          [-3.8548e-01, -1.5026e-01, -7.6040e-02]]],


        [[[-3.1344e-01,  6.9156e-02,  1.1146e-01],
          [-2.9101e-01,  3.4138e-01,  9.3271e-02],
          [ 8.2602e-03,  2.0942e-01,  2.7972e-01]]],


        [[[-3.6355e-01, -9.8365e-02,  3.2882e-01],
          [-6.8063e-02,  5.3917e-02,  8.0882e-02],
          [-1.6333e-01,  4.0149e-01,  2.8698e-01]]],


        [[[ 3.8086e-01,  6.5091e-02,  3.2578e-01],
          [-1.4561e-01,  2.7501e-01, -1.6371e-01],
          [ 1.5826e-01,  1.4048e-01, -2.1911e-01]]],


        [[[ 2.7663e-01,  3.0411e-01, -1.8270e-01],
          [ 1.0251e-01, -2.3115e-02, -2.9186e-01],
          [ 2.2537e-01, -2.6347e-01, -2.7846e-01]]],


        [[[ 1.2907e-01,  2.4934e-01,  3.7553e-01],
          [ 2.4604e-01,  2.3326e-01, -2.9987e-02],
          [ 1.3922e-01,  2.9301e-01, -2.0738e-01]]],


        [[[ 9.8661e-02, -1.7491e-02,  8.9968e-03],
          [ 2.0598e-01,  5.6547e-02,  3.7761e-01],
          [-2.8021e-01, -1.7694e-01, -2.4932e-02]]],


        [[[ 2.4616e-01, -2.8673e-01, -2.7213e-01],
          [-1.4000e-01, -3.9073e-01,  4.4300e-02],
          [-8.8992e-02, -1.9969e-01,  2.5306e-01]]],


        [[[-2.9159e-01, -2.3097e-01, -3.6997e-01],
          [-1.5458e-01, -1.4304e-01, -2.7364e-03],
          [ 2.1527e-01,  2.5703e-01,  3.2883e-02]]],


        [[[ 2.6537e-01, -1.8020e-02, -3.2031e-01],
          [-3.2984e-02,  8.0872e-02, -2.7292e-01],
          [-8.4592e-02,  1.6451e-02, -4.3469e-02]]],


        [[[ 2.1441e-01,  3.7469e-01, -1.1640e-01],
          [ 3.2611e-01,  1.4580e-01,  3.2722e-01],
          [-1.9956e-01, -2.8273e-01, -1.3855e-01]]],


        [[[ 2.2820e-01,  2.4348e-01,  2.0774e-01],
          [-5.5315e-02,  3.2912e-01,  3.0041e-01],
          [ 1.7732e-01, -3.3415e-01, -2.5708e-01]]],


        [[[-1.5884e-01,  1.4521e-01,  2.2761e-01],
          [-1.0270e-01, -3.8092e-01, -3.3882e-01],
          [ 3.0715e-02,  1.1327e-01, -5.8706e-02]]]], requires_grad=True)
---------------------------
model.conv1.bias<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([-0.2409,  0.0724,  0.0955, -0.0036,  0.1251,  0.0163,  0.1566, -0.3302,
         0.3033,  0.2697, -0.1810,  0.2523,  0.1834, -0.0104,  0.2157,  0.2366,
         0.0101,  0.0833,  0.2126,  0.3820,  0.0117, -0.0878, -0.1076,  0.0744,
        -0.0380,  0.0126,  0.1906,  0.3059, -0.3417,  0.0423, -0.0004,  0.1466],
       requires_grad=True)
---------------------------
model.conv2.weight<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([[[[ 2.2251e-02, -4.5430e-02, -2.5386e-02],
          [-2.8277e-02,  4.4625e-02, -1.9133e-02],
          [ 1.7407e-02, -2.9140e-02, -4.6816e-02]],

         [[ 6.0873e-02,  1.5478e-01,  6.5877e-02],
          [ 5.7981e-02,  2.9333e-02, -3.7363e-03],
          [-2.6730e-02, -6.8750e-02,  5.8222e-02]],

         [[-3.1369e-02, -9.1750e-03,  4.4650e-02],
          [-4.3660e-03, -1.7365e-02, -1.6397e-02],
          [-4.6239e-02, -7.9950e-02, -2.3298e-02]],

         ...,

         [[-3.7226e-02, -1.6294e-02,  6.0316e-02],
          [ 8.8456e-03,  6.7735e-02,  8.0788e-03],
          [-1.8276e-02,  3.8795e-02,  2.5336e-02]],

         [[-4.1092e-02,  5.2458e-02,  5.3392e-02],
          [ 6.5459e-02, -3.0011e-02,  4.2680e-02],
          [-8.8483e-03,  2.3632e-02, -7.5326e-02]],

         [[ 5.2193e-02, -9.7431e-03,  3.6064e-03],
          [ 6.7965e-02,  5.6165e-02,  5.7454e-02],
          [ 1.2170e-01,  3.2655e-02,  4.1076e-02]]],


        [[[ 2.4213e-02,  2.3419e-03,  5.5778e-02],
          [ 8.4009e-03, -1.3928e-02, -3.9568e-02],
          [ 2.1530e-02, -2.9712e-02, -5.2443e-02]],

         [[-3.6052e-02,  5.2014e-02, -4.8413e-02],
          [-4.5626e-02,  5.3182e-03, -7.9157e-02],
          [ 1.6329e-02, -6.7772e-02,  1.7193e-02]],

         [[ 2.3961e-02,  2.9844e-02,  6.8519e-02],
          [ 4.3712e-02,  3.6313e-02,  5.0415e-02],
          [ 2.2500e-02,  6.2322e-02,  5.4046e-02]],

         ...,

         [[-9.1779e-02,  3.1340e-02,  5.7061e-02],
          [-8.1271e-02, -3.6358e-02, -1.5579e-02],
          [ 4.0008e-02, -9.2390e-03, -4.9136e-02]],

         [[ 7.2060e-03,  9.8608e-03, -9.2604e-03],
          [ 1.6913e-02,  4.4102e-02, -3.8534e-02],
          [ 2.9955e-02, -5.2278e-03,  5.9734e-02]],

         [[-6.4326e-02, -9.8949e-02, -3.3211e-02],
          [-7.6599e-02,  1.4234e-02, -7.5904e-03],
          [-8.2197e-03, -6.9464e-02, -6.9155e-02]]],


        [[[-7.3226e-03,  5.0315e-02,  1.9218e-02],
          [ 5.3646e-03, -2.0761e-02, -5.3933e-02],
          [ 1.8797e-02,  2.9451e-02,  1.6925e-02]],

         [[-5.9609e-02,  2.1142e-03,  3.4217e-02],
          [-2.4323e-02, -2.1229e-02, -2.0598e-02],
          [-3.6279e-02,  3.4073e-02, -3.2888e-02]],

         [[-4.2278e-02, -5.6760e-02, -2.5758e-02],
          [-4.2710e-02, -3.0216e-03, -3.8033e-02],
          [-2.0301e-02, -3.5858e-02,  2.6355e-02]],

         ...,

         [[ 3.9693e-02, -2.0976e-02, -1.3860e-02],
          [ 5.2524e-02, -3.8656e-02,  5.1280e-02],
          [ 2.6810e-02, -3.7756e-02, -1.7099e-02]],

         [[ 2.8485e-02, -5.7918e-02, -2.1931e-02],
          [-8.2517e-03, -3.3711e-02,  1.9482e-02],
          [-1.5365e-02,  2.4832e-02,  4.9834e-02]],

         [[-2.7950e-03,  1.6670e-03, -3.3011e-02],
          [-3.5629e-02, -6.2907e-02,  5.2544e-03],
          [-1.4581e-02, -5.5157e-02, -3.7421e-02]]],


        ...,


        [[[-1.2765e-02,  3.1553e-02,  2.9452e-02],
          [-1.8599e-02, -5.7710e-02,  4.9460e-02],
          [-4.3315e-02,  2.0651e-02, -1.9595e-02]],

         [[ 4.5884e-02,  9.6146e-03, -1.1508e-01],
          [ 4.8294e-02, -4.2040e-02, -1.5631e-01],
          [-3.8786e-02, -8.1839e-02, -1.4454e-01]],

         [[-8.5455e-02, -3.4115e-02,  6.4393e-02],
          [-1.3080e-02, -5.3862e-02,  1.1066e-02],
          [-8.0429e-02,  4.7378e-02, -1.2956e-02]],

         ...,

         [[-9.4439e-02, -1.0833e-01, -6.1762e-02],
          [-5.3487e-02, -9.0517e-02, -4.8512e-02],
          [-8.6582e-02, -3.7222e-02, -7.6154e-02]],

         [[-1.7652e-01, -4.4356e-02, -3.6280e-03],
          [-6.8133e-02, -7.0354e-02, -5.1152e-02],
          [-5.5210e-02, -3.5989e-02,  3.4846e-02]],

         [[-5.6106e-02, -6.2926e-02, -5.5950e-02],
          [ 2.5118e-02, -1.0752e-01, -1.2758e-01],
          [-6.4627e-03, -5.0722e-02, -1.2945e-01]]],


        [[[-8.9633e-03,  4.2713e-02, -1.7727e-02],
          [-1.2763e-02,  1.4326e-02,  4.9100e-02],
          [-3.3700e-02, -6.1453e-03,  6.7158e-03]],

         [[-5.0387e-02,  3.2007e-02, -3.0527e-02],
          [ 2.8820e-02,  2.8694e-02,  2.1336e-02],
          [ 4.6339e-02,  2.9584e-02,  3.0142e-02]],

         [[-1.1583e-01,  6.4881e-03, -2.7621e-02],
          [-1.5559e-02, -1.2821e-02, -3.1000e-03],
          [ 1.6956e-02,  5.0312e-02, -1.5349e-02]],

         ...,

         [[-1.2875e-01, -1.0522e-01, -1.2133e-02],
          [-5.7523e-02, -5.6372e-02, -3.3886e-02],
          [ 7.8355e-02,  6.5530e-02,  2.1318e-02]],

         [[-1.7006e-01, -1.2965e-01, -8.1324e-02],
          [-1.4104e-01, -1.2573e-01, -1.2902e-01],
          [ 3.5625e-02, -4.4216e-04, -1.1563e-01]],

         [[ 3.0868e-02,  9.9898e-03,  3.2969e-03],
          [ 6.2428e-02,  7.0464e-02,  1.4789e-02],
          [-3.2964e-02, -2.5214e-02,  2.7996e-02]]],


        [[[ 3.4728e-02, -3.8239e-02, -3.4733e-03],
          [-3.0558e-02,  1.9031e-02, -2.0839e-02],
          [ 3.8965e-02, -1.9313e-03, -1.5811e-02]],

         [[ 7.9999e-03,  9.3312e-03, -7.6349e-02],
          [-5.1404e-02, -6.3635e-02,  3.3509e-03],
          [ 1.6181e-03,  3.5794e-02, -2.5207e-02]],

         [[ 3.3059e-02,  3.9020e-02, -9.6901e-03],
          [-1.4938e-02, -3.5016e-02, -4.9876e-02],
          [-3.9024e-02, -4.1290e-03, -4.8286e-02]],

         ...,

         [[ 7.1011e-02, -2.4446e-02,  2.7939e-02],
          [ 5.1157e-02, -2.3582e-02,  5.2506e-02],
          [ 2.6109e-02, -4.7219e-02, -1.9183e-02]],

         [[ 5.1226e-02, -3.5926e-02, -2.0594e-02],
          [ 7.0981e-02, -2.0965e-02, -1.3958e-04],
          [-5.7459e-02,  2.8277e-02, -8.3378e-02]],

         [[ 5.1026e-02,  5.6739e-02,  2.3853e-02],
          [-8.6888e-03, -8.1049e-02, -5.6273e-02],
          [ 2.6133e-02,  3.6434e-02,  3.0561e-02]]]], requires_grad=True)
---------------------------
model.conv2.bias<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([-0.0616, -0.0706, -0.0554,  0.0287, -0.0064, -0.0171, -0.0361, -0.0463,
        -0.0159,  0.0071, -0.0384,  0.0423, -0.0419, -0.0094, -0.0238,  0.0441,
         0.0172, -0.0497, -0.0217, -0.0087, -0.0494, -0.0023,  0.0062,  0.0092,
         0.0403,  0.0418,  0.0262, -0.0091,  0.0046,  0.0010, -0.0392,  0.0577,
        -0.0124,  0.0236, -0.0366, -0.0457, -0.0210,  0.0138, -0.0445, -0.0681,
        -0.0461, -0.0559,  0.0473, -0.0356, -0.0414, -0.0642, -0.0269,  0.0103,
         0.0154, -0.0047, -0.0407, -0.0656,  0.0472,  0.0462, -0.0323, -0.0071,
         0.0021,  0.0459,  0.0097, -0.0419, -0.0322,  0.0304,  0.0535, -0.0257],
       requires_grad=True)
---------------------------
model.fc1.weight<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([[ 0.0127, -0.0064,  0.0038,  ..., -0.0185,  0.0077,  0.0141],
        [ 0.0001, -0.0089,  0.0171,  ..., -0.0478, -0.0235,  0.0117],
        [ 0.0100,  0.0019, -0.0089,  ..., -0.0145, -0.0115,  0.0043],
        ...,
        [-0.0039, -0.0016,  0.0238,  ..., -0.0132, -0.0193, -0.0088],
        [-0.0103, -0.0092, -0.0098,  ...,  0.0042, -0.0060,  0.0035],
        [-0.0059,  0.0066, -0.0040,  ..., -0.0218, -0.0002, -0.0123]],
       requires_grad=True)
---------------------------
model.fc1.bias<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([-1.7558e-03, -2.5703e-02,  2.4065e-03,  1.0916e-02, -7.9553e-03,
        -1.4218e-02, -6.9479e-03,  2.0471e-02, -1.9638e-02, -7.4517e-03,
         2.1780e-03, -1.8941e-03,  2.4091e-02, -1.1227e-02, -1.4752e-02,
         3.5139e-04, -9.7976e-03, -1.2322e-02, -5.4179e-03, -1.4609e-02,
        -3.3809e-03, -4.6309e-03,  1.5808e-02, -1.3797e-02, -1.5176e-02,
         3.3732e-03, -2.1074e-02, -1.8279e-02,  1.5112e-03, -8.4515e-03,
         1.4966e-02,  1.6066e-02, -9.6311e-03,  4.2724e-05, -1.4623e-02,
         7.9703e-03, -5.2246e-03, -1.7503e-03, -1.8908e-02, -9.0204e-03,
         6.0572e-04, -1.3023e-02,  1.6581e-02, -6.4040e-03,  6.4666e-03,
         1.0487e-02,  1.1113e-02, -1.9785e-02,  1.5550e-03, -1.6939e-02,
         5.5166e-03,  7.3902e-03,  1.1096e-02, -9.6075e-03, -8.8328e-03,
        -4.7027e-04, -1.2491e-02,  4.2954e-03, -1.1657e-03, -8.8121e-03,
         1.3158e-02,  7.6111e-03, -1.0439e-02,  9.5565e-03, -3.4633e-03,
        -1.8689e-02, -7.2098e-03, -7.3144e-03, -1.2027e-02, -3.6868e-03,
         9.5429e-04, -7.1390e-03, -2.1284e-02,  2.2477e-02,  2.2013e-04,
        -3.9612e-03,  1.5451e-03, -3.1834e-03, -5.1276e-03, -1.3023e-02,
         1.7766e-02,  2.4163e-02,  2.3508e-02, -1.7727e-02, -9.1485e-05,
        -1.5576e-02, -1.1030e-02, -2.5206e-03, -1.2140e-02, -1.0998e-02,
        -2.1517e-02, -8.4767e-03,  7.2220e-03,  9.5459e-03,  2.8135e-02,
        -2.5022e-03,  1.0957e-02, -4.2475e-02, -2.1257e-02,  2.2480e-03,
        -8.1304e-04, -2.2216e-02,  3.3460e-03, -1.2243e-03, -6.6037e-04,
        -1.9586e-02, -2.0984e-02, -6.9473e-03, -2.9654e-02, -1.0762e-02,
        -1.0114e-02,  1.0635e-02,  3.2545e-04,  3.0234e-03, -5.8417e-03,
         9.8255e-03, -1.6776e-03, -1.2833e-02, -1.0766e-02, -1.4368e-02,
        -1.7255e-02, -9.2194e-03,  2.4796e-04,  4.7861e-03, -1.1339e-02,
        -9.4369e-03, -7.7872e-03, -1.7708e-03], requires_grad=True)
---------------------------
model.fc2.weight<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([[-0.1344, -0.1065, -0.0022,  ..., -0.1111,  0.0153,  0.0260],
        [-0.0899, -0.0298,  0.0323,  ...,  0.0389,  0.0042, -0.0541],
        [-0.0673, -0.0771, -0.0515,  ..., -0.0944, -0.0817, -0.0136],
        ...,
        [ 0.0498, -0.0536,  0.0760,  ..., -0.0737, -0.0126,  0.0705],
        [ 0.0152, -0.0784, -0.0312,  ...,  0.0553,  0.0222,  0.0036],
        [ 0.0325, -0.0365,  0.0105,  ...,  0.0558,  0.0487,  0.0369]],
       requires_grad=True)
---------------------------
model.fc2.bias<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([-0.0504,  0.1116,  0.0299, -0.0010,  0.0487, -0.0432,  0.0310, -0.0144,
        -0.0493,  0.0225], requires_grad=True)
---------------------------
===========================
===========================
===modules===
          <class '__main__.QNet'>       : QNet(
  (model): Net(
    (conv1): Conv2d(
      1, 32, kernel_size=(3, 3), stride=(1, 1)
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
    (conv2): Conv2d(
      32, 64, kernel_size=(3, 3), stride=(1, 1)
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
    (dropout1): Dropout(p=0.25, inplace=False)
    (dropout2): Dropout(p=0.5, inplace=False)
    (fc1): Linear(
      in_features=9216, out_features=128, bias=True
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
    (fc2): Linear(
      in_features=128, out_features=10, bias=True
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
)
---------------------------model     <class '__main__.Net'>        : Net(
  (conv1): Conv2d(
    1, 32, kernel_size=(3, 3), stride=(1, 1)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (conv2): Conv2d(
    32, 64, kernel_size=(3, 3), stride=(1, 1)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (dropout1): Dropout(p=0.25, inplace=False)
  (dropout2): Dropout(p=0.5, inplace=False)
  (fc1): Linear(
    in_features=9216, out_features=128, bias=True
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (fc2): Linear(
    in_features=128, out_features=10, bias=True
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
)
---------------------------model.conv1<class 'torch.nn.modules.conv.Conv2d'>: Conv2d(
  1, 32, kernel_size=(3, 3), stride=(1, 1)
  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
)
---------------------------model.conv1.activation_post_process<class 'torch.quantization.observer.MinMaxObserver'>: MinMaxObserver(min_val=inf, max_val=-inf)
---------------------------model.conv2<class 'torch.nn.modules.conv.Conv2d'>: Conv2d(
  32, 64, kernel_size=(3, 3), stride=(1, 1)
  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
)
---------------------------model.conv2.activation_post_process<class 'torch.quantization.observer.MinMaxObserver'>: MinMaxObserver(min_val=inf, max_val=-inf)
---------------------------model.dropout1<class 'torch.nn.modules.dropout.Dropout'>: Dropout(p=0.25, inplace=False)
---------------------------model.dropout2<class 'torch.nn.modules.dropout.Dropout'>: Dropout(p=0.5, inplace=False)
---------------------------model.fc1 <class 'torch.nn.modules.linear.Linear'>: Linear(
  in_features=9216, out_features=128, bias=True
  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
)
---------------------------model.fc1.activation_post_process<class 'torch.quantization.observer.MinMaxObserver'>: MinMaxObserver(min_val=inf, max_val=-inf)
---------------------------model.fc2 <class 'torch.nn.modules.linear.Linear'>: Linear(
  in_features=128, out_features=10, bias=True
  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
)
---------------------------model.fc2.activation_post_process<class 'torch.quantization.observer.MinMaxObserver'>: MinMaxObserver(min_val=inf, max_val=-inf)
---------------------------After converted

=====================
===params===
model.conv1.weight<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([[[[-2.9880e-01, -1.0493e-01, -9.4408e-02],
          [-4.0809e-03, -9.0901e-02, -2.3946e-01],
          [ 7.0191e-03, -2.4777e-01,  5.5444e-02]]],


        [[[-1.4003e-01,  5.9618e-02, -2.8677e-01],
          [-6.7451e-02,  1.6354e-01,  3.4268e-01],
          [ 1.2182e-01, -1.9744e-01, -1.4641e-01]]],


        [[[ 3.3644e-01,  1.5786e-01,  1.6438e-01],
          [ 2.1964e-01,  3.6397e-01,  2.4788e-02],
          [-4.1152e-02,  9.3328e-02,  2.8832e-01]]],


        [[[ 3.0041e-01, -1.5248e-01,  1.7188e-01],
          [ 3.5256e-01,  1.8857e-01, -6.5669e-02],
          [ 3.0654e-02, -2.5478e-01,  1.2163e-01]]],


        [[[ 1.2081e-01, -1.4371e-01, -4.0632e-01],
          [ 9.5525e-02,  3.0044e-01, -6.6426e-02],
          [ 4.4631e-01,  3.5866e-01,  3.6507e-01]]],


        [[[ 3.2748e-01,  3.0948e-02,  2.6961e-01],
          [ 5.1997e-02, -1.9818e-01,  4.2359e-01],
          [-3.5331e-01, -2.7336e-01,  1.6425e-01]]],


        [[[ 1.3710e-01, -1.0606e-02,  4.5137e-02],
          [ 1.6811e-01,  2.9117e-01,  3.2487e-01],
          [-2.9893e-01, -2.7467e-01, -4.1834e-01]]],


        [[[-1.4843e-01, -2.3468e-01,  9.4389e-02],
          [-6.8487e-02, -1.2644e-01, -1.9385e-02],
          [-3.3218e-01, -1.5136e-01, -7.6472e-02]]],


        [[[ 1.6482e-01,  3.1565e-01,  3.7848e-02],
          [ 1.7079e-01,  3.0124e-01, -8.2340e-02],
          [-6.2145e-02, -3.6574e-01, -3.9719e-01]]],


        [[[-1.2120e-01, -4.2474e-01, -3.7961e-04],
          [ 2.2245e-03, -3.5148e-01,  1.1253e-01],
          [-3.5961e-01, -1.7102e-01,  3.4422e-01]]],


        [[[ 9.2521e-02, -2.3701e-01, -1.0267e-01],
          [-2.2923e-01,  5.2078e-02, -1.4016e-01],
          [-3.0949e-01, -3.2445e-01,  1.1963e-01]]],


        [[[-9.6144e-02,  2.5533e-01,  2.6186e-01],
          [-3.7505e-01, -1.6370e-01,  4.4318e-01],
          [-3.1788e-01, -1.7731e-01,  4.0111e-01]]],


        [[[ 1.1919e-01,  1.3126e-01,  1.7953e-02],
          [-3.5331e-02,  1.2237e-01,  2.0066e-01],
          [-1.8288e-01, -4.3487e-01,  3.4378e-02]]],


        [[[ 7.6056e-02,  3.4161e-01,  1.3664e-01],
          [-1.6394e-01, -1.3505e-01, -1.1138e-01],
          [ 7.2916e-02,  6.5297e-02,  2.2525e-01]]],


        [[[-3.3831e-01, -4.5490e-01, -3.6624e-01],
          [ 1.4136e-01,  1.7022e-01,  1.3857e-01],
          [ 1.9665e-01,  2.7525e-01, -9.9629e-02]]],


        [[[-5.0057e-02, -3.6314e-02,  1.2715e-01],
          [-2.7514e-01,  7.9140e-02,  2.9155e-01],
          [ 1.2797e-01, -3.5860e-01,  1.2535e-01]]],


        [[[ 5.7037e-02, -2.3822e-01,  5.7111e-02],
          [ 1.0360e-01,  1.5539e-01,  6.3637e-02],
          [ 1.5844e-01,  2.7834e-01, -1.7206e-01]]],


        [[[-8.9117e-02, -1.6373e-02,  1.7731e-01],
          [ 3.0791e-01,  9.3516e-02,  3.1589e-01],
          [ 9.7591e-02,  1.6531e-01,  7.1941e-02]]],


        [[[-2.3436e-01,  2.6186e-02, -1.0402e-01],
          [-3.6678e-01,  1.3026e-01, -2.9739e-01],
          [-1.5570e-01, -1.0743e-01, -8.0777e-02]]],


        [[[-1.1433e-02, -2.4903e-01,  1.3219e-01],
          [-2.8103e-01, -1.7838e-01, -9.1438e-02],
          [-3.8548e-01, -1.5026e-01, -7.6040e-02]]],


        [[[-3.1344e-01,  6.9156e-02,  1.1146e-01],
          [-2.9101e-01,  3.4138e-01,  9.3271e-02],
          [ 8.2602e-03,  2.0942e-01,  2.7972e-01]]],


        [[[-3.6355e-01, -9.8365e-02,  3.2882e-01],
          [-6.8063e-02,  5.3917e-02,  8.0882e-02],
          [-1.6333e-01,  4.0149e-01,  2.8698e-01]]],


        [[[ 3.8086e-01,  6.5091e-02,  3.2578e-01],
          [-1.4561e-01,  2.7501e-01, -1.6371e-01],
          [ 1.5826e-01,  1.4048e-01, -2.1911e-01]]],


        [[[ 2.7663e-01,  3.0411e-01, -1.8270e-01],
          [ 1.0251e-01, -2.3115e-02, -2.9186e-01],
          [ 2.2537e-01, -2.6347e-01, -2.7846e-01]]],


        [[[ 1.2907e-01,  2.4934e-01,  3.7553e-01],
          [ 2.4604e-01,  2.3326e-01, -2.9987e-02],
          [ 1.3922e-01,  2.9301e-01, -2.0738e-01]]],


        [[[ 9.8661e-02, -1.7491e-02,  8.9968e-03],
          [ 2.0598e-01,  5.6547e-02,  3.7761e-01],
          [-2.8021e-01, -1.7694e-01, -2.4932e-02]]],


        [[[ 2.4616e-01, -2.8673e-01, -2.7213e-01],
          [-1.4000e-01, -3.9073e-01,  4.4300e-02],
          [-8.8992e-02, -1.9969e-01,  2.5306e-01]]],


        [[[-2.9159e-01, -2.3097e-01, -3.6997e-01],
          [-1.5458e-01, -1.4304e-01, -2.7364e-03],
          [ 2.1527e-01,  2.5703e-01,  3.2883e-02]]],


        [[[ 2.6537e-01, -1.8020e-02, -3.2031e-01],
          [-3.2984e-02,  8.0872e-02, -2.7292e-01],
          [-8.4592e-02,  1.6451e-02, -4.3469e-02]]],


        [[[ 2.1441e-01,  3.7469e-01, -1.1640e-01],
          [ 3.2611e-01,  1.4580e-01,  3.2722e-01],
          [-1.9956e-01, -2.8273e-01, -1.3855e-01]]],


        [[[ 2.2820e-01,  2.4348e-01,  2.0774e-01],
          [-5.5315e-02,  3.2912e-01,  3.0041e-01],
          [ 1.7732e-01, -3.3415e-01, -2.5708e-01]]],


        [[[-1.5884e-01,  1.4521e-01,  2.2761e-01],
          [-1.0270e-01, -3.8092e-01, -3.3882e-01],
          [ 3.0715e-02,  1.1327e-01, -5.8706e-02]]]], requires_grad=True)
---------------------------
model.conv1.bias<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([-0.2409,  0.0724,  0.0955, -0.0036,  0.1251,  0.0163,  0.1566, -0.3302,
         0.3033,  0.2697, -0.1810,  0.2523,  0.1834, -0.0104,  0.2157,  0.2366,
         0.0101,  0.0833,  0.2126,  0.3820,  0.0117, -0.0878, -0.1076,  0.0744,
        -0.0380,  0.0126,  0.1906,  0.3059, -0.3417,  0.0423, -0.0004,  0.1466],
       requires_grad=True)
---------------------------
model.conv2.weight<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([[[[ 2.2251e-02, -4.5430e-02, -2.5386e-02],
          [-2.8277e-02,  4.4625e-02, -1.9133e-02],
          [ 1.7407e-02, -2.9140e-02, -4.6816e-02]],

         [[ 6.0873e-02,  1.5478e-01,  6.5877e-02],
          [ 5.7981e-02,  2.9333e-02, -3.7363e-03],
          [-2.6730e-02, -6.8750e-02,  5.8222e-02]],

         [[-3.1369e-02, -9.1750e-03,  4.4650e-02],
          [-4.3660e-03, -1.7365e-02, -1.6397e-02],
          [-4.6239e-02, -7.9950e-02, -2.3298e-02]],

         ...,

         [[-3.7226e-02, -1.6294e-02,  6.0316e-02],
          [ 8.8456e-03,  6.7735e-02,  8.0788e-03],
          [-1.8276e-02,  3.8795e-02,  2.5336e-02]],

         [[-4.1092e-02,  5.2458e-02,  5.3392e-02],
          [ 6.5459e-02, -3.0011e-02,  4.2680e-02],
          [-8.8483e-03,  2.3632e-02, -7.5326e-02]],

         [[ 5.2193e-02, -9.7431e-03,  3.6064e-03],
          [ 6.7965e-02,  5.6165e-02,  5.7454e-02],
          [ 1.2170e-01,  3.2655e-02,  4.1076e-02]]],


        [[[ 2.4213e-02,  2.3419e-03,  5.5778e-02],
          [ 8.4009e-03, -1.3928e-02, -3.9568e-02],
          [ 2.1530e-02, -2.9712e-02, -5.2443e-02]],

         [[-3.6052e-02,  5.2014e-02, -4.8413e-02],
          [-4.5626e-02,  5.3182e-03, -7.9157e-02],
          [ 1.6329e-02, -6.7772e-02,  1.7193e-02]],

         [[ 2.3961e-02,  2.9844e-02,  6.8519e-02],
          [ 4.3712e-02,  3.6313e-02,  5.0415e-02],
          [ 2.2500e-02,  6.2322e-02,  5.4046e-02]],

         ...,

         [[-9.1779e-02,  3.1340e-02,  5.7061e-02],
          [-8.1271e-02, -3.6358e-02, -1.5579e-02],
          [ 4.0008e-02, -9.2390e-03, -4.9136e-02]],

         [[ 7.2060e-03,  9.8608e-03, -9.2604e-03],
          [ 1.6913e-02,  4.4102e-02, -3.8534e-02],
          [ 2.9955e-02, -5.2278e-03,  5.9734e-02]],

         [[-6.4326e-02, -9.8949e-02, -3.3211e-02],
          [-7.6599e-02,  1.4234e-02, -7.5904e-03],
          [-8.2197e-03, -6.9464e-02, -6.9155e-02]]],


        [[[-7.3226e-03,  5.0315e-02,  1.9218e-02],
          [ 5.3646e-03, -2.0761e-02, -5.3933e-02],
          [ 1.8797e-02,  2.9451e-02,  1.6925e-02]],

         [[-5.9609e-02,  2.1142e-03,  3.4217e-02],
          [-2.4323e-02, -2.1229e-02, -2.0598e-02],
          [-3.6279e-02,  3.4073e-02, -3.2888e-02]],

         [[-4.2278e-02, -5.6760e-02, -2.5758e-02],
          [-4.2710e-02, -3.0216e-03, -3.8033e-02],
          [-2.0301e-02, -3.5858e-02,  2.6355e-02]],

         ...,

         [[ 3.9693e-02, -2.0976e-02, -1.3860e-02],
          [ 5.2524e-02, -3.8656e-02,  5.1280e-02],
          [ 2.6810e-02, -3.7756e-02, -1.7099e-02]],

         [[ 2.8485e-02, -5.7918e-02, -2.1931e-02],
          [-8.2517e-03, -3.3711e-02,  1.9482e-02],
          [-1.5365e-02,  2.4832e-02,  4.9834e-02]],

         [[-2.7950e-03,  1.6670e-03, -3.3011e-02],
          [-3.5629e-02, -6.2907e-02,  5.2544e-03],
          [-1.4581e-02, -5.5157e-02, -3.7421e-02]]],


        ...,


        [[[-1.2765e-02,  3.1553e-02,  2.9452e-02],
          [-1.8599e-02, -5.7710e-02,  4.9460e-02],
          [-4.3315e-02,  2.0651e-02, -1.9595e-02]],

         [[ 4.5884e-02,  9.6146e-03, -1.1508e-01],
          [ 4.8294e-02, -4.2040e-02, -1.5631e-01],
          [-3.8786e-02, -8.1839e-02, -1.4454e-01]],

         [[-8.5455e-02, -3.4115e-02,  6.4393e-02],
          [-1.3080e-02, -5.3862e-02,  1.1066e-02],
          [-8.0429e-02,  4.7378e-02, -1.2956e-02]],

         ...,

         [[-9.4439e-02, -1.0833e-01, -6.1762e-02],
          [-5.3487e-02, -9.0517e-02, -4.8512e-02],
          [-8.6582e-02, -3.7222e-02, -7.6154e-02]],

         [[-1.7652e-01, -4.4356e-02, -3.6280e-03],
          [-6.8133e-02, -7.0354e-02, -5.1152e-02],
          [-5.5210e-02, -3.5989e-02,  3.4846e-02]],

         [[-5.6106e-02, -6.2926e-02, -5.5950e-02],
          [ 2.5118e-02, -1.0752e-01, -1.2758e-01],
          [-6.4627e-03, -5.0722e-02, -1.2945e-01]]],


        [[[-8.9633e-03,  4.2713e-02, -1.7727e-02],
          [-1.2763e-02,  1.4326e-02,  4.9100e-02],
          [-3.3700e-02, -6.1453e-03,  6.7158e-03]],

         [[-5.0387e-02,  3.2007e-02, -3.0527e-02],
          [ 2.8820e-02,  2.8694e-02,  2.1336e-02],
          [ 4.6339e-02,  2.9584e-02,  3.0142e-02]],

         [[-1.1583e-01,  6.4881e-03, -2.7621e-02],
          [-1.5559e-02, -1.2821e-02, -3.1000e-03],
          [ 1.6956e-02,  5.0312e-02, -1.5349e-02]],

         ...,

         [[-1.2875e-01, -1.0522e-01, -1.2133e-02],
          [-5.7523e-02, -5.6372e-02, -3.3886e-02],
          [ 7.8355e-02,  6.5530e-02,  2.1318e-02]],

         [[-1.7006e-01, -1.2965e-01, -8.1324e-02],
          [-1.4104e-01, -1.2573e-01, -1.2902e-01],
          [ 3.5625e-02, -4.4216e-04, -1.1563e-01]],

         [[ 3.0868e-02,  9.9898e-03,  3.2969e-03],
          [ 6.2428e-02,  7.0464e-02,  1.4789e-02],
          [-3.2964e-02, -2.5214e-02,  2.7996e-02]]],


        [[[ 3.4728e-02, -3.8239e-02, -3.4733e-03],
          [-3.0558e-02,  1.9031e-02, -2.0839e-02],
          [ 3.8965e-02, -1.9313e-03, -1.5811e-02]],

         [[ 7.9999e-03,  9.3312e-03, -7.6349e-02],
          [-5.1404e-02, -6.3635e-02,  3.3509e-03],
          [ 1.6181e-03,  3.5794e-02, -2.5207e-02]],

         [[ 3.3059e-02,  3.9020e-02, -9.6901e-03],
          [-1.4938e-02, -3.5016e-02, -4.9876e-02],
          [-3.9024e-02, -4.1290e-03, -4.8286e-02]],

         ...,

         [[ 7.1011e-02, -2.4446e-02,  2.7939e-02],
          [ 5.1157e-02, -2.3582e-02,  5.2506e-02],
          [ 2.6109e-02, -4.7219e-02, -1.9183e-02]],

         [[ 5.1226e-02, -3.5926e-02, -2.0594e-02],
          [ 7.0981e-02, -2.0965e-02, -1.3958e-04],
          [-5.7459e-02,  2.8277e-02, -8.3378e-02]],

         [[ 5.1026e-02,  5.6739e-02,  2.3853e-02],
          [-8.6888e-03, -8.1049e-02, -5.6273e-02],
          [ 2.6133e-02,  3.6434e-02,  3.0561e-02]]]], requires_grad=True)
---------------------------
model.conv2.bias<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([-0.0616, -0.0706, -0.0554,  0.0287, -0.0064, -0.0171, -0.0361, -0.0463,
        -0.0159,  0.0071, -0.0384,  0.0423, -0.0419, -0.0094, -0.0238,  0.0441,
         0.0172, -0.0497, -0.0217, -0.0087, -0.0494, -0.0023,  0.0062,  0.0092,
         0.0403,  0.0418,  0.0262, -0.0091,  0.0046,  0.0010, -0.0392,  0.0577,
        -0.0124,  0.0236, -0.0366, -0.0457, -0.0210,  0.0138, -0.0445, -0.0681,
        -0.0461, -0.0559,  0.0473, -0.0356, -0.0414, -0.0642, -0.0269,  0.0103,
         0.0154, -0.0047, -0.0407, -0.0656,  0.0472,  0.0462, -0.0323, -0.0071,
         0.0021,  0.0459,  0.0097, -0.0419, -0.0322,  0.0304,  0.0535, -0.0257],
       requires_grad=True)
---------------------------
model.fc1.weight<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([[ 0.0127, -0.0064,  0.0038,  ..., -0.0185,  0.0077,  0.0141],
        [ 0.0001, -0.0089,  0.0171,  ..., -0.0478, -0.0235,  0.0117],
        [ 0.0100,  0.0019, -0.0089,  ..., -0.0145, -0.0115,  0.0043],
        ...,
        [-0.0039, -0.0016,  0.0238,  ..., -0.0132, -0.0193, -0.0088],
        [-0.0103, -0.0092, -0.0098,  ...,  0.0042, -0.0060,  0.0035],
        [-0.0059,  0.0066, -0.0040,  ..., -0.0218, -0.0002, -0.0123]],
       requires_grad=True)
---------------------------
model.fc1.bias<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([-1.7558e-03, -2.5703e-02,  2.4065e-03,  1.0916e-02, -7.9553e-03,
        -1.4218e-02, -6.9479e-03,  2.0471e-02, -1.9638e-02, -7.4517e-03,
         2.1780e-03, -1.8941e-03,  2.4091e-02, -1.1227e-02, -1.4752e-02,
         3.5139e-04, -9.7976e-03, -1.2322e-02, -5.4179e-03, -1.4609e-02,
        -3.3809e-03, -4.6309e-03,  1.5808e-02, -1.3797e-02, -1.5176e-02,
         3.3732e-03, -2.1074e-02, -1.8279e-02,  1.5112e-03, -8.4515e-03,
         1.4966e-02,  1.6066e-02, -9.6311e-03,  4.2724e-05, -1.4623e-02,
         7.9703e-03, -5.2246e-03, -1.7503e-03, -1.8908e-02, -9.0204e-03,
         6.0572e-04, -1.3023e-02,  1.6581e-02, -6.4040e-03,  6.4666e-03,
         1.0487e-02,  1.1113e-02, -1.9785e-02,  1.5550e-03, -1.6939e-02,
         5.5166e-03,  7.3902e-03,  1.1096e-02, -9.6075e-03, -8.8328e-03,
        -4.7027e-04, -1.2491e-02,  4.2954e-03, -1.1657e-03, -8.8121e-03,
         1.3158e-02,  7.6111e-03, -1.0439e-02,  9.5565e-03, -3.4633e-03,
        -1.8689e-02, -7.2098e-03, -7.3144e-03, -1.2027e-02, -3.6868e-03,
         9.5429e-04, -7.1390e-03, -2.1284e-02,  2.2477e-02,  2.2013e-04,
        -3.9612e-03,  1.5451e-03, -3.1834e-03, -5.1276e-03, -1.3023e-02,
         1.7766e-02,  2.4163e-02,  2.3508e-02, -1.7727e-02, -9.1485e-05,
        -1.5576e-02, -1.1030e-02, -2.5206e-03, -1.2140e-02, -1.0998e-02,
        -2.1517e-02, -8.4767e-03,  7.2220e-03,  9.5459e-03,  2.8135e-02,
        -2.5022e-03,  1.0957e-02, -4.2475e-02, -2.1257e-02,  2.2480e-03,
        -8.1304e-04, -2.2216e-02,  3.3460e-03, -1.2243e-03, -6.6037e-04,
        -1.9586e-02, -2.0984e-02, -6.9473e-03, -2.9654e-02, -1.0762e-02,
        -1.0114e-02,  1.0635e-02,  3.2545e-04,  3.0234e-03, -5.8417e-03,
         9.8255e-03, -1.6776e-03, -1.2833e-02, -1.0766e-02, -1.4368e-02,
        -1.7255e-02, -9.2194e-03,  2.4796e-04,  4.7861e-03, -1.1339e-02,
        -9.4369e-03, -7.7872e-03, -1.7708e-03], requires_grad=True)
---------------------------
model.fc2.weight<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([[-0.1344, -0.1065, -0.0022,  ..., -0.1111,  0.0153,  0.0260],
        [-0.0899, -0.0298,  0.0323,  ...,  0.0389,  0.0042, -0.0541],
        [-0.0673, -0.0771, -0.0515,  ..., -0.0944, -0.0817, -0.0136],
        ...,
        [ 0.0498, -0.0536,  0.0760,  ..., -0.0737, -0.0126,  0.0705],
        [ 0.0152, -0.0784, -0.0312,  ...,  0.0553,  0.0222,  0.0036],
        [ 0.0325, -0.0365,  0.0105,  ...,  0.0558,  0.0487,  0.0369]],
       requires_grad=True)
---------------------------
model.fc2.bias<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([-0.0504,  0.1116,  0.0299, -0.0010,  0.0487, -0.0432,  0.0310, -0.0144,
        -0.0493,  0.0225], requires_grad=True)
---------------------------
===========================
===========================
===modules===
          <class '__main__.QNet'>       : QNet(
  (model): Net(
    (conv1): Conv2d(
      1, 32, kernel_size=(3, 3), stride=(1, 1)
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
    (conv2): Conv2d(
      32, 64, kernel_size=(3, 3), stride=(1, 1)
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
    (dropout1): Dropout(p=0.25, inplace=False)
    (dropout2): Dropout(p=0.5, inplace=False)
    (fc1): Linear(
      in_features=9216, out_features=128, bias=True
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
    (fc2): Linear(
      in_features=128, out_features=10, bias=True
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
)
---------------------------model     <class '__main__.Net'>        : Net(
  (conv1): Conv2d(
    1, 32, kernel_size=(3, 3), stride=(1, 1)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (conv2): Conv2d(
    32, 64, kernel_size=(3, 3), stride=(1, 1)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (dropout1): Dropout(p=0.25, inplace=False)
  (dropout2): Dropout(p=0.5, inplace=False)
  (fc1): Linear(
    in_features=9216, out_features=128, bias=True
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (fc2): Linear(
    in_features=128, out_features=10, bias=True
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
)
---------------------------model.conv1<class 'torch.nn.modules.conv.Conv2d'>: Conv2d(
  1, 32, kernel_size=(3, 3), stride=(1, 1)
  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
)
---------------------------model.conv1.activation_post_process<class 'torch.quantization.observer.MinMaxObserver'>: MinMaxObserver(min_val=inf, max_val=-inf)
---------------------------model.conv2<class 'torch.nn.modules.conv.Conv2d'>: Conv2d(
  32, 64, kernel_size=(3, 3), stride=(1, 1)
  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
)
---------------------------model.conv2.activation_post_process<class 'torch.quantization.observer.MinMaxObserver'>: MinMaxObserver(min_val=inf, max_val=-inf)
---------------------------model.dropout1<class 'torch.nn.modules.dropout.Dropout'>: Dropout(p=0.25, inplace=False)
---------------------------model.dropout2<class 'torch.nn.modules.dropout.Dropout'>: Dropout(p=0.5, inplace=False)
---------------------------model.fc1 <class 'torch.nn.modules.linear.Linear'>: Linear(
  in_features=9216, out_features=128, bias=True
  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
)
---------------------------model.fc1.activation_post_process<class 'torch.quantization.observer.MinMaxObserver'>: MinMaxObserver(min_val=inf, max_val=-inf)
---------------------------model.fc2 <class 'torch.nn.modules.linear.Linear'>: Linear(
  in_features=128, out_features=10, bias=True
  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
)
---------------------------model.fc2.activation_post_process<class 'torch.quantization.observer.MinMaxObserver'>: MinMaxObserver(min_val=inf, max_val=-inf)
---------------------------After converted

=====================
===params===
===========================
===========================
===modules===
          <class '__main__.QNet'>       : QNet(
  (model): Net(
    (conv1): QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
    (conv2): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
    (dropout1): Dropout(p=0.25, inplace=False)
    (dropout2): Dropout(p=0.5, inplace=False)
    (fc1): QuantizedLinear(in_features=9216, out_features=128, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
    (fc2): QuantizedLinear(in_features=128, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
  )
)
---------------------------model     <class '__main__.Net'>        : Net(
  (conv1): QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
  (conv2): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
  (dropout1): Dropout(p=0.25, inplace=False)
  (dropout2): Dropout(p=0.5, inplace=False)
  (fc1): QuantizedLinear(in_features=9216, out_features=128, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
  (fc2): QuantizedLinear(in_features=128, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
)
---------------------------model.conv1<class 'torch.nn.quantized.modules.conv.Conv2d'>: QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
---------------------------model.conv2<class 'torch.nn.quantized.modules.conv.Conv2d'>: QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
---------------------------model.dropout1<class 'torch.nn.modules.dropout.Dropout'>: Dropout(p=0.25, inplace=False)
---------------------------model.dropout2<class 'torch.nn.modules.dropout.Dropout'>: Dropout(p=0.5, inplace=False)
---------------------------model.fc1 <class 'torch.nn.quantized.modules.linear.Linear'>: QuantizedLinear(in_features=9216, out_features=128, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
---------------------------model.fc1._packed_params<class 'torch.nn.quantized.modules.linear.LinearPackedParams'>: (tensor([[ 0.0127, -0.0070,  0.0042,  ..., -0.0183,  0.0070,  0.0141],
        [ 0.0000, -0.0084,  0.0169,  ..., -0.0479, -0.0239,  0.0113],
        [ 0.0099,  0.0014, -0.0084,  ..., -0.0141, -0.0113,  0.0042],
        ...,
        [-0.0042, -0.0014,  0.0239,  ..., -0.0127, -0.0197, -0.0084],
        [-0.0099, -0.0099, -0.0099,  ...,  0.0042, -0.0056,  0.0028],
        [-0.0056,  0.0070, -0.0042,  ..., -0.0211,  0.0000, -0.0127]],
       size=(128, 9216), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0014075974468141794,
       zero_point=0), tensor([-1.7558e-03, -2.5703e-02,  2.4065e-03,  1.0916e-02, -7.9553e-03,
        -1.4218e-02, -6.9479e-03,  2.0471e-02, -1.9638e-02, -7.4517e-03,
         2.1780e-03, -1.8941e-03,  2.4091e-02, -1.1227e-02, -1.4752e-02,
         3.5139e-04, -9.7976e-03, -1.2322e-02, -5.4179e-03, -1.4609e-02,
        -3.3809e-03, -4.6309e-03,  1.5808e-02, -1.3797e-02, -1.5176e-02,
         3.3732e-03, -2.1074e-02, -1.8279e-02,  1.5112e-03, -8.4515e-03,
         1.4966e-02,  1.6066e-02, -9.6311e-03,  4.2724e-05, -1.4623e-02,
         7.9703e-03, -5.2246e-03, -1.7503e-03, -1.8908e-02, -9.0204e-03,
         6.0572e-04, -1.3023e-02,  1.6581e-02, -6.4040e-03,  6.4666e-03,
         1.0487e-02,  1.1113e-02, -1.9785e-02,  1.5550e-03, -1.6939e-02,
         5.5166e-03,  7.3902e-03,  1.1096e-02, -9.6075e-03, -8.8328e-03,
        -4.7027e-04, -1.2491e-02,  4.2954e-03, -1.1657e-03, -8.8121e-03,
         1.3158e-02,  7.6111e-03, -1.0439e-02,  9.5565e-03, -3.4633e-03,
        -1.8689e-02, -7.2098e-03, -7.3144e-03, -1.2027e-02, -3.6868e-03,
         9.5429e-04, -7.1390e-03, -2.1284e-02,  2.2477e-02,  2.2013e-04,
        -3.9612e-03,  1.5451e-03, -3.1834e-03, -5.1276e-03, -1.3023e-02,
         1.7766e-02,  2.4163e-02,  2.3508e-02, -1.7727e-02, -9.1485e-05,
        -1.5576e-02, -1.1030e-02, -2.5206e-03, -1.2140e-02, -1.0998e-02,
        -2.1517e-02, -8.4767e-03,  7.2220e-03,  9.5459e-03,  2.8135e-02,
        -2.5022e-03,  1.0957e-02, -4.2475e-02, -2.1257e-02,  2.2480e-03,
        -8.1304e-04, -2.2216e-02,  3.3460e-03, -1.2243e-03, -6.6037e-04,
        -1.9586e-02, -2.0984e-02, -6.9473e-03, -2.9654e-02, -1.0762e-02,
        -1.0114e-02,  1.0635e-02,  3.2545e-04,  3.0234e-03, -5.8417e-03,
         9.8255e-03, -1.6776e-03, -1.2833e-02, -1.0766e-02, -1.4368e-02,
        -1.7255e-02, -9.2194e-03,  2.4796e-04,  4.7861e-03, -1.1339e-02,
        -9.4369e-03, -7.7872e-03, -1.7708e-03], requires_grad=True))
---------------------------model.fc2 <class 'torch.nn.quantized.modules.linear.Linear'>: QuantizedLinear(in_features=128, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
---------------------------model.fc2._packed_params<class 'torch.nn.quantized.modules.linear.LinearPackedParams'>: (tensor([[-0.1342, -0.1070, -0.0015,  ..., -0.1116,  0.0151,  0.0256],
        [-0.0904, -0.0301,  0.0317,  ...,  0.0392,  0.0045, -0.0543],
        [-0.0678, -0.0769, -0.0513,  ..., -0.0950, -0.0814, -0.0136],
        ...,
        [ 0.0497, -0.0543,  0.0754,  ..., -0.0739, -0.0121,  0.0708],
        [ 0.0151, -0.0784, -0.0317,  ...,  0.0558,  0.0226,  0.0030],
        [ 0.0332, -0.0362,  0.0106,  ...,  0.0558,  0.0482,  0.0362]],
       size=(10, 128), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0015074362745508552,
       zero_point=0), tensor([-0.0504,  0.1116,  0.0299, -0.0010,  0.0487, -0.0432,  0.0310, -0.0144,
        -0.0493,  0.0225], requires_grad=True))
---------------------------After converted

=====================
===params===
===========================
===========================
===modules===
          <class '__main__.QNet'>       : QNet(
  (model): Net(
    (conv1): QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
    (conv2): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
    (dropout1): Dropout(p=0.25, inplace=False)
    (dropout2): Dropout(p=0.5, inplace=False)
    (fc1): QuantizedLinear(in_features=9216, out_features=128, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
    (fc2): QuantizedLinear(in_features=128, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
  )
)
---------------------------model     <class '__main__.Net'>        : Net(
  (conv1): QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
  (conv2): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
  (dropout1): Dropout(p=0.25, inplace=False)
  (dropout2): Dropout(p=0.5, inplace=False)
  (fc1): QuantizedLinear(in_features=9216, out_features=128, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
  (fc2): QuantizedLinear(in_features=128, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
)
---------------------------model.conv1<class 'torch.nn.quantized.modules.conv.Conv2d'>: QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
---------------------------model.conv2<class 'torch.nn.quantized.modules.conv.Conv2d'>: QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
---------------------------model.dropout1<class 'torch.nn.modules.dropout.Dropout'>: Dropout(p=0.25, inplace=False)
---------------------------model.dropout2<class 'torch.nn.modules.dropout.Dropout'>: Dropout(p=0.5, inplace=False)
---------------------------model.fc1 <class 'torch.nn.quantized.modules.linear.Linear'>: QuantizedLinear(in_features=9216, out_features=128, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
---------------------------model.fc1._packed_params<class 'torch.nn.quantized.modules.linear.LinearPackedParams'>: (tensor([[ 0.0127, -0.0070,  0.0042,  ..., -0.0183,  0.0070,  0.0141],
        [ 0.0000, -0.0084,  0.0169,  ..., -0.0479, -0.0239,  0.0113],
        [ 0.0099,  0.0014, -0.0084,  ..., -0.0141, -0.0113,  0.0042],
        ...,
        [-0.0042, -0.0014,  0.0239,  ..., -0.0127, -0.0197, -0.0084],
        [-0.0099, -0.0099, -0.0099,  ...,  0.0042, -0.0056,  0.0028],
        [-0.0056,  0.0070, -0.0042,  ..., -0.0211,  0.0000, -0.0127]],
       size=(128, 9216), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0014075974468141794,
       zero_point=0), tensor([-1.7558e-03, -2.5703e-02,  2.4065e-03,  1.0916e-02, -7.9553e-03,
        -1.4218e-02, -6.9479e-03,  2.0471e-02, -1.9638e-02, -7.4517e-03,
         2.1780e-03, -1.8941e-03,  2.4091e-02, -1.1227e-02, -1.4752e-02,
         3.5139e-04, -9.7976e-03, -1.2322e-02, -5.4179e-03, -1.4609e-02,
        -3.3809e-03, -4.6309e-03,  1.5808e-02, -1.3797e-02, -1.5176e-02,
         3.3732e-03, -2.1074e-02, -1.8279e-02,  1.5112e-03, -8.4515e-03,
         1.4966e-02,  1.6066e-02, -9.6311e-03,  4.2724e-05, -1.4623e-02,
         7.9703e-03, -5.2246e-03, -1.7503e-03, -1.8908e-02, -9.0204e-03,
         6.0572e-04, -1.3023e-02,  1.6581e-02, -6.4040e-03,  6.4666e-03,
         1.0487e-02,  1.1113e-02, -1.9785e-02,  1.5550e-03, -1.6939e-02,
         5.5166e-03,  7.3902e-03,  1.1096e-02, -9.6075e-03, -8.8328e-03,
        -4.7027e-04, -1.2491e-02,  4.2954e-03, -1.1657e-03, -8.8121e-03,
         1.3158e-02,  7.6111e-03, -1.0439e-02,  9.5565e-03, -3.4633e-03,
        -1.8689e-02, -7.2098e-03, -7.3144e-03, -1.2027e-02, -3.6868e-03,
         9.5429e-04, -7.1390e-03, -2.1284e-02,  2.2477e-02,  2.2013e-04,
        -3.9612e-03,  1.5451e-03, -3.1834e-03, -5.1276e-03, -1.3023e-02,
         1.7766e-02,  2.4163e-02,  2.3508e-02, -1.7727e-02, -9.1485e-05,
        -1.5576e-02, -1.1030e-02, -2.5206e-03, -1.2140e-02, -1.0998e-02,
        -2.1517e-02, -8.4767e-03,  7.2220e-03,  9.5459e-03,  2.8135e-02,
        -2.5022e-03,  1.0957e-02, -4.2475e-02, -2.1257e-02,  2.2480e-03,
        -8.1304e-04, -2.2216e-02,  3.3460e-03, -1.2243e-03, -6.6037e-04,
        -1.9586e-02, -2.0984e-02, -6.9473e-03, -2.9654e-02, -1.0762e-02,
        -1.0114e-02,  1.0635e-02,  3.2545e-04,  3.0234e-03, -5.8417e-03,
         9.8255e-03, -1.6776e-03, -1.2833e-02, -1.0766e-02, -1.4368e-02,
        -1.7255e-02, -9.2194e-03,  2.4796e-04,  4.7861e-03, -1.1339e-02,
        -9.4369e-03, -7.7872e-03, -1.7708e-03], requires_grad=True))
---------------------------model.fc2 <class 'torch.nn.quantized.modules.linear.Linear'>: QuantizedLinear(in_features=128, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
---------------------------model.fc2._packed_params<class 'torch.nn.quantized.modules.linear.LinearPackedParams'>: (tensor([[-0.1342, -0.1070, -0.0015,  ..., -0.1116,  0.0151,  0.0256],
        [-0.0904, -0.0301,  0.0317,  ...,  0.0392,  0.0045, -0.0543],
        [-0.0678, -0.0769, -0.0513,  ..., -0.0950, -0.0814, -0.0136],
        ...,
        [ 0.0497, -0.0543,  0.0754,  ..., -0.0739, -0.0121,  0.0708],
        [ 0.0151, -0.0784, -0.0317,  ...,  0.0558,  0.0226,  0.0030],
        [ 0.0332, -0.0362,  0.0106,  ...,  0.0558,  0.0482,  0.0362]],
       size=(10, 128), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0015074362745508552,
       zero_point=0), tensor([-0.0504,  0.1116,  0.0299, -0.0010,  0.0487, -0.0432,  0.0310, -0.0144,
        -0.0493,  0.0225], requires_grad=True))
---------------------------After converted

=====================
===params===
===========================
===========================
===modules===
          <class '__main__.QNet'>       : QNet(
  (model): Net(
    (conv1): QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
    (conv2): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
    (dropout1): Dropout(p=0.25, inplace=False)
    (dropout2): Dropout(p=0.5, inplace=False)
    (fc1): QuantizedLinear(in_features=9216, out_features=128, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
    (fc2): QuantizedLinear(in_features=128, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
  )
)
---------------------------model     <class '__main__.Net'>        : Net(
  (conv1): QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
  (conv2): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
  (dropout1): Dropout(p=0.25, inplace=False)
  (dropout2): Dropout(p=0.5, inplace=False)
  (fc1): QuantizedLinear(in_features=9216, out_features=128, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
  (fc2): QuantizedLinear(in_features=128, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
)
---------------------------model.conv1<class 'torch.nn.quantized.modules.conv.Conv2d'>: QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
---------------------------model.conv2<class 'torch.nn.quantized.modules.conv.Conv2d'>: QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
---------------------------model.dropout1<class 'torch.nn.modules.dropout.Dropout'>: Dropout(p=0.25, inplace=False)
---------------------------model.dropout2<class 'torch.nn.modules.dropout.Dropout'>: Dropout(p=0.5, inplace=False)
---------------------------model.fc1 <class 'torch.nn.quantized.modules.linear.Linear'>: QuantizedLinear(in_features=9216, out_features=128, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
---------------------------model.fc1._packed_params<class 'torch.nn.quantized.modules.linear.LinearPackedParams'>: (tensor([[ 0.0127, -0.0070,  0.0042,  ..., -0.0183,  0.0070,  0.0141],
        [ 0.0000, -0.0084,  0.0169,  ..., -0.0479, -0.0239,  0.0113],
        [ 0.0099,  0.0014, -0.0084,  ..., -0.0141, -0.0113,  0.0042],
        ...,
        [-0.0042, -0.0014,  0.0239,  ..., -0.0127, -0.0197, -0.0084],
        [-0.0099, -0.0099, -0.0099,  ...,  0.0042, -0.0056,  0.0028],
        [-0.0056,  0.0070, -0.0042,  ..., -0.0211,  0.0000, -0.0127]],
       size=(128, 9216), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0014075974468141794,
       zero_point=0), tensor([-1.7558e-03, -2.5703e-02,  2.4065e-03,  1.0916e-02, -7.9553e-03,
        -1.4218e-02, -6.9479e-03,  2.0471e-02, -1.9638e-02, -7.4517e-03,
         2.1780e-03, -1.8941e-03,  2.4091e-02, -1.1227e-02, -1.4752e-02,
         3.5139e-04, -9.7976e-03, -1.2322e-02, -5.4179e-03, -1.4609e-02,
        -3.3809e-03, -4.6309e-03,  1.5808e-02, -1.3797e-02, -1.5176e-02,
         3.3732e-03, -2.1074e-02, -1.8279e-02,  1.5112e-03, -8.4515e-03,
         1.4966e-02,  1.6066e-02, -9.6311e-03,  4.2724e-05, -1.4623e-02,
         7.9703e-03, -5.2246e-03, -1.7503e-03, -1.8908e-02, -9.0204e-03,
         6.0572e-04, -1.3023e-02,  1.6581e-02, -6.4040e-03,  6.4666e-03,
         1.0487e-02,  1.1113e-02, -1.9785e-02,  1.5550e-03, -1.6939e-02,
         5.5166e-03,  7.3902e-03,  1.1096e-02, -9.6075e-03, -8.8328e-03,
        -4.7027e-04, -1.2491e-02,  4.2954e-03, -1.1657e-03, -8.8121e-03,
         1.3158e-02,  7.6111e-03, -1.0439e-02,  9.5565e-03, -3.4633e-03,
        -1.8689e-02, -7.2098e-03, -7.3144e-03, -1.2027e-02, -3.6868e-03,
         9.5429e-04, -7.1390e-03, -2.1284e-02,  2.2477e-02,  2.2013e-04,
        -3.9612e-03,  1.5451e-03, -3.1834e-03, -5.1276e-03, -1.3023e-02,
         1.7766e-02,  2.4163e-02,  2.3508e-02, -1.7727e-02, -9.1485e-05,
        -1.5576e-02, -1.1030e-02, -2.5206e-03, -1.2140e-02, -1.0998e-02,
        -2.1517e-02, -8.4767e-03,  7.2220e-03,  9.5459e-03,  2.8135e-02,
        -2.5022e-03,  1.0957e-02, -4.2475e-02, -2.1257e-02,  2.2480e-03,
        -8.1304e-04, -2.2216e-02,  3.3460e-03, -1.2243e-03, -6.6037e-04,
        -1.9586e-02, -2.0984e-02, -6.9473e-03, -2.9654e-02, -1.0762e-02,
        -1.0114e-02,  1.0635e-02,  3.2545e-04,  3.0234e-03, -5.8417e-03,
         9.8255e-03, -1.6776e-03, -1.2833e-02, -1.0766e-02, -1.4368e-02,
        -1.7255e-02, -9.2194e-03,  2.4796e-04,  4.7861e-03, -1.1339e-02,
        -9.4369e-03, -7.7872e-03, -1.7708e-03], requires_grad=True))
---------------------------model.fc2 <class 'torch.nn.quantized.modules.linear.Linear'>: QuantizedLinear(in_features=128, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
---------------------------model.fc2._packed_params<class 'torch.nn.quantized.modules.linear.LinearPackedParams'>: (tensor([[-0.1342, -0.1070, -0.0015,  ..., -0.1116,  0.0151,  0.0256],
        [-0.0904, -0.0301,  0.0317,  ...,  0.0392,  0.0045, -0.0543],
        [-0.0678, -0.0769, -0.0513,  ..., -0.0950, -0.0814, -0.0136],
        ...,
        [ 0.0497, -0.0543,  0.0754,  ..., -0.0739, -0.0121,  0.0708],
        [ 0.0151, -0.0784, -0.0317,  ...,  0.0558,  0.0226,  0.0030],
        [ 0.0332, -0.0362,  0.0106,  ...,  0.0558,  0.0482,  0.0362]],
       size=(10, 128), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0015074362745508552,
       zero_point=0), tensor([-0.0504,  0.1116,  0.0299, -0.0010,  0.0487, -0.0432,  0.0310, -0.0144,
        -0.0493,  0.0225], requires_grad=True))
---------------------------After converted

=====================
===params===
===========================
===========================
===modules===
          <class '__main__.QNet'>       : QNet(
  (model): Net(
    (conv1): QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
    (conv2): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
    (dropout1): Dropout(p=0.25, inplace=False)
    (dropout2): Dropout(p=0.5, inplace=False)
    (fc1): QuantizedLinear(in_features=9216, out_features=128, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
    (fc2): QuantizedLinear(in_features=128, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
  )
)
---------------------------model     <class '__main__.Net'>        : Net(
  (conv1): QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
  (conv2): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
  (dropout1): Dropout(p=0.25, inplace=False)
  (dropout2): Dropout(p=0.5, inplace=False)
  (fc1): QuantizedLinear(in_features=9216, out_features=128, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
  (fc2): QuantizedLinear(in_features=128, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
)
---------------------------model.conv1<class 'torch.nn.quantized.modules.conv.Conv2d'>: QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
---------------------------model.conv2<class 'torch.nn.quantized.modules.conv.Conv2d'>: QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)
---------------------------model.dropout1<class 'torch.nn.modules.dropout.Dropout'>: Dropout(p=0.25, inplace=False)
---------------------------model.dropout2<class 'torch.nn.modules.dropout.Dropout'>: Dropout(p=0.5, inplace=False)
---------------------------model.fc1 <class 'torch.nn.quantized.modules.linear.Linear'>: QuantizedLinear(in_features=9216, out_features=128, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
---------------------------model.fc1._packed_params<class 'torch.nn.quantized.modules.linear.LinearPackedParams'>: (tensor([[ 0.0127, -0.0070,  0.0042,  ..., -0.0183,  0.0070,  0.0141],
        [ 0.0000, -0.0084,  0.0169,  ..., -0.0479, -0.0239,  0.0113],
        [ 0.0099,  0.0014, -0.0084,  ..., -0.0141, -0.0113,  0.0042],
        ...,
        [-0.0042, -0.0014,  0.0239,  ..., -0.0127, -0.0197, -0.0084],
        [-0.0099, -0.0099, -0.0099,  ...,  0.0042, -0.0056,  0.0028],
        [-0.0056,  0.0070, -0.0042,  ..., -0.0211,  0.0000, -0.0127]],
       size=(128, 9216), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0014075974468141794,
       zero_point=0), tensor([-1.7558e-03, -2.5703e-02,  2.4065e-03,  1.0916e-02, -7.9553e-03,
        -1.4218e-02, -6.9479e-03,  2.0471e-02, -1.9638e-02, -7.4517e-03,
         2.1780e-03, -1.8941e-03,  2.4091e-02, -1.1227e-02, -1.4752e-02,
         3.5139e-04, -9.7976e-03, -1.2322e-02, -5.4179e-03, -1.4609e-02,
        -3.3809e-03, -4.6309e-03,  1.5808e-02, -1.3797e-02, -1.5176e-02,
         3.3732e-03, -2.1074e-02, -1.8279e-02,  1.5112e-03, -8.4515e-03,
         1.4966e-02,  1.6066e-02, -9.6311e-03,  4.2724e-05, -1.4623e-02,
         7.9703e-03, -5.2246e-03, -1.7503e-03, -1.8908e-02, -9.0204e-03,
         6.0572e-04, -1.3023e-02,  1.6581e-02, -6.4040e-03,  6.4666e-03,
         1.0487e-02,  1.1113e-02, -1.9785e-02,  1.5550e-03, -1.6939e-02,
         5.5166e-03,  7.3902e-03,  1.1096e-02, -9.6075e-03, -8.8328e-03,
        -4.7027e-04, -1.2491e-02,  4.2954e-03, -1.1657e-03, -8.8121e-03,
         1.3158e-02,  7.6111e-03, -1.0439e-02,  9.5565e-03, -3.4633e-03,
        -1.8689e-02, -7.2098e-03, -7.3144e-03, -1.2027e-02, -3.6868e-03,
         9.5429e-04, -7.1390e-03, -2.1284e-02,  2.2477e-02,  2.2013e-04,
        -3.9612e-03,  1.5451e-03, -3.1834e-03, -5.1276e-03, -1.3023e-02,
         1.7766e-02,  2.4163e-02,  2.3508e-02, -1.7727e-02, -9.1485e-05,
        -1.5576e-02, -1.1030e-02, -2.5206e-03, -1.2140e-02, -1.0998e-02,
        -2.1517e-02, -8.4767e-03,  7.2220e-03,  9.5459e-03,  2.8135e-02,
        -2.5022e-03,  1.0957e-02, -4.2475e-02, -2.1257e-02,  2.2480e-03,
        -8.1304e-04, -2.2216e-02,  3.3460e-03, -1.2243e-03, -6.6037e-04,
        -1.9586e-02, -2.0984e-02, -6.9473e-03, -2.9654e-02, -1.0762e-02,
        -1.0114e-02,  1.0635e-02,  3.2545e-04,  3.0234e-03, -5.8417e-03,
         9.8255e-03, -1.6776e-03, -1.2833e-02, -1.0766e-02, -1.4368e-02,
        -1.7255e-02, -9.2194e-03,  2.4796e-04,  4.7861e-03, -1.1339e-02,
        -9.4369e-03, -7.7872e-03, -1.7708e-03], requires_grad=True))
---------------------------model.fc2 <class 'torch.nn.quantized.modules.linear.Linear'>: QuantizedLinear(in_features=128, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
---------------------------model.fc2._packed_params<class 'torch.nn.quantized.modules.linear.LinearPackedParams'>: (tensor([[-0.1342, -0.1070, -0.0015,  ..., -0.1116,  0.0151,  0.0256],
        [-0.0904, -0.0301,  0.0317,  ...,  0.0392,  0.0045, -0.0543],
        [-0.0678, -0.0769, -0.0513,  ..., -0.0950, -0.0814, -0.0136],
        ...,
        [ 0.0497, -0.0543,  0.0754,  ..., -0.0739, -0.0121,  0.0708],
        [ 0.0151, -0.0784, -0.0317,  ...,  0.0558,  0.0226,  0.0030],
        [ 0.0332, -0.0362,  0.0106,  ...,  0.0558,  0.0482,  0.0362]],
       size=(10, 128), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0015074362745508552,
       zero_point=0), tensor([-0.0504,  0.1116,  0.0299, -0.0010,  0.0487, -0.0432,  0.0310, -0.0144,
        -0.0493,  0.0225], requires_grad=True))
---------------------------